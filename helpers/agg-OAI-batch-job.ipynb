{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv \n",
    "load_dotenv()\n",
    "import os\n",
    "import ast\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "gc.disable()\n",
    "from copy import deepcopy as cc\n",
    "import pandas as pd\n",
    "from utils import (get_data, process_facts_df, process_posts_df, \n",
    "                   get_openai_embedding, get_sbert_embeddings, get_openai_batch_ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key = os.getenv('OPENAI_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job for file at input: openai-ip/eval_orig_fact_large.jsonl; batch_input_file_id : file-xICI12hBFg6dzX3UAt3ETho5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 1/8 [00:00<00:06,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job for file at input: openai-ip/eval_l2_post_large_df.jsonl; batch_input_file_id : file-hlbiycVtn15O0acamZavQfTi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 2/8 [00:01<00:05,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job for file at input: openai-ip/eval_l2_post_df.jsonl; batch_input_file_id : file-yRPQttjDsfNdp7rzwOUfwJHP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 3/8 [00:02<00:04,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job for file at input: openai-ip/eval_l1_post_large_df.jsonl; batch_input_file_id : file-bebSPgvlyXbAwR7tYaFl5m5e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 4/8 [00:03<00:03,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job for file at input: openai-ip/eval_orig_fact.jsonl; batch_input_file_id : file-vehlLcFIHhEwviazr8GZduTH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 5/8 [00:04<00:02,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job for file at input: openai-ip/eval_eng_fact.jsonl; batch_input_file_id : file-eC3CL8vHYr3NHvssDlrDMDEZ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 6/8 [00:05<00:01,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job for file at input: openai-ip/eval_l1_post_df.jsonl; batch_input_file_id : file-nxaOPVKJfH5Ycg4q9koBcLjl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 7/8 [00:06<00:00,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job for file at input: openai-ip/eval_eng_fact_large.jsonl; batch_input_file_id : file-9hut5s7xVXyLJQPg4sRinFVZ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:06<00:00,  1.15it/s]\n"
     ]
    }
   ],
   "source": [
    "# Retrieving the openai jobs response for embedding the post and fact data\n",
    "batches = []\n",
    "\n",
    "for batch in tqdm(client.batches.list(limit=8).data, total=len(client.batches.list(limit=8).data)):\n",
    "    batch_id = batch.id\n",
    "    print(batch.metadata['description'])\n",
    "    if 'job for file' in batch.metadata['description']:\n",
    "        batches.append(batch)\n",
    "        op_file_id = client.batches.retrieve(batch_id).output_file_id\n",
    "        file_response = client.files.content(op_file_id)\n",
    "        dst_path = batch.metadata['description'].split(';')[0].split(':')[-1].strip().replace('openai-ip', 'openai-op')\n",
    "        file_response.write_to_file(dst_path)\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in ['orig', 'eng']:\n",
    "    df_final = []\n",
    "    for ix in tqdm(range(5)):\n",
    "        file_large = f\"{lang}_fact_large_{ix}.jsonl\"\n",
    "        file_small = f\"{lang}_fact_{ix}.jsonl\"\n",
    "\n",
    "        df = pd.read_json(os.path.join('openai-op', file_small), lines = True)\n",
    "        df_large = pd.read_json(os.path.join('openai-op', file_large), lines = True)\n",
    "\n",
    "        df[f'gpt_small_emb'] = df.apply(lambda x: x['response']['body']['data'][0]['embedding'], axis = 1)\n",
    "        df_large[f'gpt_large_emb'] = df_large.apply(lambda x: x['response']['body']['data'][0]['embedding'], axis = 1)\n",
    "\n",
    "        df['fact_check_id'] = df['custom_id']\n",
    "        df_large['fact_check_id'] = df_large['custom_id']\n",
    "\n",
    "        columns = ['response', 'custom_id', 'error', 'id']\n",
    "        df.drop(columns, inplace=True, axis=1)\n",
    "        df_large.drop(columns, inplace=True, axis=1)\n",
    "\n",
    "        df_large = df_large.merge(df)\n",
    "        del df\n",
    "        df_final.append(df_large)\n",
    "    df_final = pd.concat(df_final)\n",
    "    df_final.to_pickle(f'openai-op/{lang}-fact.pkl', protocol = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in ['orig', 'eng']:\n",
    "    file_large = f\"eval_{lang}_fact_large.jsonl\"\n",
    "    file_small = f\"eval_{lang}_fact.jsonl\"\n",
    "\n",
    "    df = pd.read_json(os.path.join('openai-op', file_small), lines = True)\n",
    "    df_large = pd.read_json(os.path.join('openai-op', file_large), lines = True)\n",
    "\n",
    "    df[f'gpt_small_emb'] = df.apply(lambda x: x['response']['body']['data'][0]['embedding'], axis = 1)\n",
    "    df_large[f'gpt_large_emb'] = df_large.apply(lambda x: x['response']['body']['data'][0]['embedding'], axis = 1)\n",
    "\n",
    "    df['fact_check_id'] = df['custom_id']\n",
    "    df_large['fact_check_id'] = df_large['custom_id']\n",
    "\n",
    "    columns = ['response', 'custom_id', 'error', 'id']\n",
    "    df.drop(columns, inplace=True, axis=1)\n",
    "    df_large.drop(columns, inplace=True, axis=1)\n",
    "\n",
    "    df_large = df_large.merge(df)\n",
    "    df_large.to_pickle(f'openai-op/eval_{lang}-fact.pkl', protocol = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.3 s, sys: 14.8 s, total: 41.1 s\n",
      "Wall time: 41.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_final = pd.read_pickle('openai-op/orig-fact.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00, 30.80it/s]\n"
     ]
    }
   ],
   "source": [
    "for ix, lang in tqdm(enumerate(['l1', 'l2'])):\n",
    "    df_final = []\n",
    "    file_large = f\"eval_{lang}_post_large_df.jsonl\"\n",
    "    file_small = f\"eval_{lang}_post_df.jsonl\"\n",
    "\n",
    "    df = pd.read_json(os.path.join('openai-op', file_small), lines = True)\n",
    "    df_large = pd.read_json(os.path.join('openai-op', file_large), lines = True)\n",
    "\n",
    "    df[f'gpt_small_emb'] = df.apply(lambda x: x['response']['body']['data'][0]['embedding'], axis = 1)\n",
    "    df_large[f'gpt_large_emb'] = df_large.apply(lambda x: x['response']['body']['data'][0]['embedding'], axis = 1)\n",
    "\n",
    "    df['post_id'] = df['custom_id']\n",
    "    df_large['post_id'] = df_large['custom_id']\n",
    "\n",
    "    columns = ['response', 'custom_id', 'error', 'id']\n",
    "    df.drop(columns, inplace=True, axis=1)\n",
    "    df_large.drop(columns, inplace=True, axis=1)\n",
    "\n",
    "    df_large = df_large.merge(df)\n",
    "    del df\n",
    "    df_large.to_pickle(f'openai-op/eval_{lang}-post.pkl', protocol = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlingual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
