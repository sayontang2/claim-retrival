{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Task**: previously fact-checked claim retrieval\n",
    "- **Input**: \n",
    "    - input claim\n",
    "    -  set of fact-checked claims\n",
    "- **Goal**: rank the fact-checked claims so that those that are the most relevant w.r.t. the input claim (and thus the most useful from the fact-checker’s perspective) are ranked as high as possible \n",
    "\n",
    "- **MultiClaim**: a novel multilingual dataset for PFCR. The dataset consists of \n",
    "    - $205,751$ $fact-checks$ in 39 languages \n",
    "    - $28,092$ social media posts (from now on just $posts$) in $27$ languages. \n",
    "    - We collected these assignments and gathered 31,305 pairs consisting of a $post$ and a $fact-check$ reviewing the claim made in the post.\n",
    "    - 4,212 of these $pairs$ are crosslingual (i.e., the language of the $fact-check$ and the language of the $post$ are different)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/mlingual/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3259"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv \n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import ast\n",
    "import gc\n",
    "import numpy as np\n",
    "gc.disable()\n",
    "from copy import deepcopy as cc\n",
    "import pytorch_lightning as pl\n",
    "import tiktoken\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from utils import (get_data, process_facts_df, process_posts_df, \n",
    "                   get_openai_embedding, get_sbert_embeddings, get_openai_batch_ip)\n",
    "import pdb\n",
    "load_dotenv()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key = os.getenv('OPENAI_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_facts, eval_posts, eval_mapping = get_data(fact_path=\"./../sample_data/trial_fact_checks.csv\",\n",
    "                                                posts_path=\"./../sample_data/trial_posts.csv\",\n",
    "                                                post2fact_mapping_path=\"./../sample_data/trial_data_mapping.csv\")\n",
    "eval_facts = process_facts_df(eval_facts)\n",
    "eval_posts = process_posts_df(eval_posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get sbert embeddings for the posts & facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at sentence-transformers/use-cmlm-multilingual were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SentenceTransformer('sentence-transformers/use-cmlm-multilingual')\n",
    "model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 155.10it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 252.99it/s]\n",
      "100%|██████████| 47/47 [00:00<00:00, 204.15it/s]\n",
      "100%|██████████| 47/47 [00:00<00:00, 209.30it/s]\n"
     ]
    }
   ],
   "source": [
    "eval_fact_orig_sbert_emb = get_sbert_embeddings(txt_list = eval_facts.facts_orig.tolist(), model=model, batch_size=128)\n",
    "eval_fact_eng_sbert_emb = get_sbert_embeddings(txt_list = eval_facts.facts_eng.tolist(), model=model, batch_size=128)\n",
    "\n",
    "eval_post_l1_sbert_emb = get_sbert_embeddings(txt_list = eval_posts.post_l1.tolist(), model=model, batch_size=128)\n",
    "eval_post_l2_sbert_emb = get_sbert_embeddings(txt_list = eval_posts.post_l2.tolist(), model=model, batch_size=128)\n",
    "\n",
    "eval_facts['fact_orig_sbert'] = eval_fact_orig_sbert_emb\n",
    "eval_facts['fact_eng_sbert'] = eval_fact_eng_sbert_emb\n",
    "\n",
    "eval_posts['post_l1_sbert'] = eval_post_l1_sbert_emb\n",
    "eval_posts['post_l2_sbert'] = eval_post_l2_sbert_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:13<00:00,  3.71it/s]\n",
      "100%|██████████| 50/50 [00:13<00:00,  3.67it/s]\n",
      "100%|██████████| 47/47 [00:13<00:00,  3.47it/s]\n",
      "100%|██████████| 47/47 [00:12<00:00,  3.76it/s]\n"
     ]
    }
   ],
   "source": [
    "mdl = 'text-embedding-3-large'\n",
    "comb_txt_list = eval_facts.apply(lambda x: f\"{x['facts_orig'].strip()} <sep> {x['facts_eng'].strip()}\", axis = 1).tolist()\n",
    "eval_fact_orig_sbert_emb = get_openai_embedding(text_list = eval_facts.facts_orig.tolist(), model=mdl, client=client)\n",
    "eval_fact_eng_sbert_emb = get_openai_embedding(text_list = eval_facts.facts_eng.tolist(), model=mdl, client=client)\n",
    "\n",
    "comb_txt_list = eval_posts.apply(lambda x: f\"{x['post_l1'].strip()} <sep> {x['post_l2'].strip()}\", axis = 1).tolist()\n",
    "eval_post_l1_sbert_emb = get_openai_embedding(text_list = eval_posts.post_l1.tolist(), model=mdl, client=client)\n",
    "eval_post_l2_sbert_emb = get_openai_embedding(text_list = eval_posts.post_l2.tolist(), model=mdl, client=client)\n",
    "\n",
    "eval_facts['fact_orig_gpt_large'] = eval_fact_orig_sbert_emb\n",
    "eval_facts['fact_eng_gpt_large'] = eval_fact_eng_sbert_emb\n",
    "\n",
    "eval_posts['post_l1_gpt_large'] = eval_post_l1_sbert_emb\n",
    "eval_posts['post_l2_gpt_large'] = eval_post_l2_sbert_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:09<00:00,  5.07it/s]\n",
      "100%|██████████| 50/50 [00:09<00:00,  5.53it/s]\n",
      "100%|██████████| 47/47 [00:09<00:00,  4.85it/s]\n",
      "100%|██████████| 47/47 [00:09<00:00,  5.13it/s]\n"
     ]
    }
   ],
   "source": [
    "mdl = 'text-embedding-3-small'\n",
    "comb_txt_list = eval_facts.apply(lambda x: f\"{x['facts_orig'].strip()} <sep> {x['facts_eng'].strip()}\", axis = 1).tolist()\n",
    "eval_fact_orig_sbert_emb = get_openai_embedding(text_list = eval_facts.facts_orig.tolist(), model=mdl, client=client)\n",
    "eval_fact_eng_sbert_emb = get_openai_embedding(text_list = eval_facts.facts_eng.tolist(), model=mdl, client=client)\n",
    "\n",
    "comb_txt_list = eval_posts.apply(lambda x: f\"{x['post_l1'].strip()} <sep> {x['post_l2'].strip()}\", axis = 1).tolist()\n",
    "eval_post_l1_sbert_emb = get_openai_embedding(text_list = eval_posts.post_l1.tolist(), model=mdl, client=client)\n",
    "eval_post_l2_sbert_emb = get_openai_embedding(text_list = eval_posts.post_l2.tolist(), model=mdl, client=client)\n",
    "\n",
    "eval_facts['fact_orig_gpt_small'] = eval_fact_orig_sbert_emb\n",
    "eval_facts['fact_eng_gpt_small'] = eval_fact_eng_sbert_emb\n",
    "\n",
    "eval_posts['post_l1_gpt_small'] = eval_post_l1_sbert_emb\n",
    "eval_posts['post_l2_gpt_small'] = eval_post_l2_sbert_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "eval_posts = eval_posts.merge(eval_mapping, on='post_id', how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mdl in ['gpt_small', 'gpt_large', 'sbert']:\n",
    "    for post_lang in ['l1', 'l2']:\n",
    "        for fact_lang in ['orig', 'eng']:\n",
    "            temp = cosine_similarity(eval_posts[f'post_{post_lang}_{mdl}'].tolist(), eval_facts[f'fact_{fact_lang}_{mdl}'].tolist())\n",
    "            eval_posts[f'{mdl}_p_{post_lang}_f_{fact_lang}'] = temp.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['post_instances', 'post_ocr', 'post_verdicts', 'post_text', 'post_id',\n",
       "       'post_l1', 'post_l2', 'l1', 'l2', 'post_l1_sbert', 'post_l2_sbert',\n",
       "       'post_l1_gpt_large', 'post_l2_gpt_large', 'post_l1_gpt_small',\n",
       "       'post_l2_gpt_small', 'fact_check_id', 'pair_lang',\n",
       "       'gpt_small_p_l1_f_orig', 'gpt_small_p_l1_f_eng',\n",
       "       'gpt_small_p_l2_f_orig', 'gpt_small_p_l2_f_eng',\n",
       "       'gpt_large_p_l1_f_orig', 'gpt_large_p_l1_f_eng',\n",
       "       'gpt_large_p_l2_f_orig', 'gpt_large_p_l2_f_eng', 'sbert_p_l1_f_orig',\n",
       "       'sbert_p_l1_f_eng', 'sbert_p_l2_f_orig', 'sbert_p_l2_f_eng'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = eval_posts.columns.tolist()\n",
    "\n",
    "# Taking the mean of cosine similarity for each model\n",
    "for mdl in ['p_l1_f_orig', 'p_l2_f_eng']:\n",
    "    eval_posts[f'mean_{mdl}'] = eval_posts.apply(lambda x: np.concatenate([np.array(x[c]).reshape(1, -1) for c in cols if c.endswith(mdl)]).mean(axis = 0), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_ix2id_mapping = {ix: row['fact_check_id'] for ix, row in eval_facts.iterrows()}\n",
    "get_post_id = lambda x: fact_ix2id_mapping[x]\n",
    "vfunc = np.vectorize(get_post_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result = {}\n",
    "cols = ['gpt_small_p_l1_f_orig','gpt_small_p_l2_f_eng',\n",
    "        'gpt_large_p_l1_f_orig', 'gpt_large_p_l2_f_eng',\n",
    "        'sbert_p_l1_f_orig', 'sbert_p_l2_f_eng',\n",
    "        'mean_p_l1_f_orig', 'mean_p_l2_f_eng']    \n",
    "for col in cols:\n",
    "    res = []\n",
    "    for k in [1, 2, 3, 4 ,5]:\n",
    "        res.append(eval_posts.apply(lambda x: x['fact_check_id'] in vfunc(np.argsort(x[col])[::-1][:k]), axis = 1).mean())\n",
    "    final_result[col] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = ['p_l1_f_orig','p_l2_f_eng'] \n",
    "\n",
    "for setting in settings:\n",
    "    columns = [c for c in eval_posts.columns.tolist() if c.endswith(setting) and not c.startswith('mean')]\n",
    "    res = []\n",
    "    for k in [1, 2, 3, 4 ,5]:\n",
    "        res.append(eval_posts.apply(lambda x: x['fact_check_id'] in  np.concatenate([vfunc(np.argsort(x[col])[::-1][:k]) for col in columns]), axis = 1).mean())\n",
    "    final_result[f'optimal_{setting}'] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gpt_small</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt_large</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sbert</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optimal</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0     1     2     3     4\n",
       "gpt_small  0.80  0.82  0.94  0.94  0.94\n",
       "gpt_large  0.86  0.92  0.94  0.94  0.96\n",
       "sbert      0.60  0.70  0.78  0.82  0.84\n",
       "mean       0.84  0.90  0.96  0.96  0.96\n",
       "optimal    0.90  0.94  0.96  0.96  0.96"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({k.split('_p')[0]: v for k, v in final_result.items() if k.endswith('p_l1_f_orig')}).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gpt_small</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt_large</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sbert</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optimal</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0     1     2     3     4\n",
       "gpt_small  0.86  0.92  0.94  0.94  0.94\n",
       "gpt_large  0.86  0.90  0.94  0.96  0.98\n",
       "sbert      0.70  0.82  0.84  0.86  0.86\n",
       "mean       0.86  0.92  0.94  0.94  0.94\n",
       "optimal    0.90  0.94  0.96  0.96  0.98"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({k.split('_p')[0]: v for k, v in final_result.items() if k.endswith('p_l2_f_eng')}).transpose()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlingual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
