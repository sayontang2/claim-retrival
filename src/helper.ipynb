{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/mlingual/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers.optimization import get_linear_schedule_with_warmup\n",
    "from transformers import logging\n",
    "logging.set_verbosity_warning()\n",
    "from transformers import AutoTokenizer\n",
    "from data_utils import *\n",
    "from model_utils import *\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "import numpy as np\n",
    "gc.disable()\n",
    "from eval import evaluate\n",
    "import pdb\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(42).shape == []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "model_name = 'sentence-transformers/use-cmlm-multilingual'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = ['A man jumped over the bridge', 'A man jumped over the bridge. A man jumped over the bridge. ']\n",
    "txt_tok = [tokenizer.encode_plus(t, truncation=True, max_length=121, return_tensors='pt') for t in txt]\n",
    "\n",
    "input_ids = [item['input_ids'].reshape(-1) for item in txt_tok]\n",
    "input_ids = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   101,    138,  15351, 281182,  15444,  14985,  71426,    102,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0],\n",
       "        [   101,    138,  15351, 281182,  15444,  14985,  71426,    119,    138,\n",
       "          15351, 281182,  15444,  14985,  71426,    119,    102]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mpad_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msequences\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbatch_first\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpadding_value\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Pad a list of variable length Tensors with ``padding_value``.\n",
      "\n",
      "``pad_sequence`` stacks a list of Tensors along a new dimension,\n",
      "and pads them to equal length. For example, if the input is a list of\n",
      "sequences with size ``L x *`` and ``batch_first`` is False, the output is\n",
      "of size ``T x B x *``.\n",
      "\n",
      "`B` is batch size. It is equal to the number of elements in ``sequences``.\n",
      "`T` is length of the longest sequence.\n",
      "`L` is length of the sequence.\n",
      "`*` is any number of trailing dimensions, including none.\n",
      "\n",
      "Example:\n",
      "    >>> from torch.nn.utils.rnn import pad_sequence\n",
      "    >>> a = torch.ones(25, 300)\n",
      "    >>> b = torch.ones(22, 300)\n",
      "    >>> c = torch.ones(15, 300)\n",
      "    >>> pad_sequence([a, b, c]).size()\n",
      "    torch.Size([25, 3, 300])\n",
      "\n",
      "Note:\n",
      "    This function returns a Tensor of size ``T x B x *`` or ``B x T x *``\n",
      "    where `T` is the length of the longest sequence. This function assumes\n",
      "    trailing dimensions and type of all the Tensors in sequences are same.\n",
      "\n",
      "Args:\n",
      "    sequences (list[Tensor]): list of variable length sequences.\n",
      "    batch_first (bool, optional): output will be in ``B x T x *`` if True, or in\n",
      "        ``T x B x *`` otherwise. Default: False.\n",
      "    padding_value (float, optional): value for padded elements. Default: 0.\n",
      "\n",
      "Returns:\n",
      "    Tensor of size ``T x B x *`` if :attr:`batch_first` is ``False``.\n",
      "    Tensor of size ``B x T x *`` otherwise\n",
      "\u001b[0;31mFile:\u001b[0m      ~/miniconda3/envs/mlingual/lib/python3.11/site-packages/torch/nn/utils/rnn.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "pad_sequence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# global variable declaration\n",
    "# ----------------------------------------------------\n",
    "model_name = 'sentence-transformers/use-cmlm-multilingual'\n",
    "external_dim1 = 1536 \n",
    "external_dim2 = 3072\n",
    "device = 'cuda:0'\n",
    "scale = 20\n",
    "batch_size = 32\n",
    "max_seq_len = 312\n",
    "eval_data_root = './../sample_data'\n",
    "train_data_root = './../in_data'\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at sentence-transformers/use-cmlm-multilingual were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodel_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FeatureSet, AggregationMethod, ModifiedSentenceTransformer\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# ----------------------------------------------------\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Model declaration\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# ----------------------------------------------------\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m model_attention \u001b[38;5;241m=\u001b[39m \u001b[43mModifiedSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                                      \u001b[49m\u001b[43mexternal_dim1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexternal_dim1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                                                      \u001b[49m\u001b[43mexternal_dim2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexternal_dim2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                                                      \u001b[49m\u001b[43maggregation_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAggregationMethod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLINEAR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                                                      \u001b[49m\u001b[43mfeature_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFeatureSet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSBERT_EXT1_EXT2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n",
      "File \u001b[0;32m~/sayontan/semeval_2025/src/model_utils.py:86\u001b[0m, in \u001b[0;36mModifiedSentenceTransformer.__init__\u001b[0;34m(self, model_name, external_dim1, external_dim2, feature_set, aggregation_method, num_heads)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_name, external_dim1, external_dim2, \n\u001b[1;32m     85\u001b[0m              feature_set\u001b[38;5;241m=\u001b[39mFeatureSet\u001b[38;5;241m.\u001b[39mSBERT_EXT1_EXT2, aggregation_method\u001b[38;5;241m=\u001b[39mAggregationMethod\u001b[38;5;241m.\u001b[39mLINEAR, num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m):\n\u001b[0;32m---> 86\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mModifiedSentenceTransformer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msbert \u001b[38;5;241m=\u001b[39m SentenceTransformer(model_name)\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msbert\u001b[38;5;241m.\u001b[39mget_sentence_embedding_dimension()\n",
      "File \u001b[0;32m~/miniconda3/envs/mlingual/lib/python3.11/site-packages/sentence_transformers/SentenceTransformer.py:287\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[0;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data)\u001b[0m\n\u001b[1;32m    278\u001b[0m         model_name_or_path \u001b[38;5;241m=\u001b[39m __MODEL_HUB_ORGANIZATION__ \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m model_name_or_path\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sentence_transformer_model(\n\u001b[1;32m    281\u001b[0m     model_name_or_path,\n\u001b[1;32m    282\u001b[0m     token,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    285\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    286\u001b[0m ):\n\u001b[0;32m--> 287\u001b[0m     modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_sbert_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    299\u001b[0m     modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_auto_model(\n\u001b[1;32m    300\u001b[0m         model_name_or_path,\n\u001b[1;32m    301\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    308\u001b[0m         config_kwargs\u001b[38;5;241m=\u001b[39mconfig_kwargs,\n\u001b[1;32m    309\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/mlingual/lib/python3.11/site-packages/sentence_transformers/SentenceTransformer.py:1487\u001b[0m, in \u001b[0;36mSentenceTransformer._load_sbert_model\u001b[0;34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code, local_files_only, model_kwargs, tokenizer_kwargs, config_kwargs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config_kwargs:\n\u001b[1;32m   1485\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig_args\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mupdate(config_kwargs)\n\u001b[0;32m-> 1487\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43mTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1488\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1489\u001b[0m     \u001b[38;5;66;03m# Normalize does not require any files to be loaded\u001b[39;00m\n\u001b[1;32m   1490\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module_class \u001b[38;5;241m==\u001b[39m Normalize:\n",
      "File \u001b[0;32m~/miniconda3/envs/mlingual/lib/python3.11/site-packages/sentence_transformers/models/Transformer.py:58\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[0;34m(self, model_name_or_path, max_seq_length, model_args, tokenizer_args, config_args, cache_dir, do_lower_case, tokenizer_name_or_path)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_seq_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_max_length\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tokenizer_args:\n\u001b[1;32m     57\u001b[0m     tokenizer_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_max_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m max_seq_length\n\u001b[0;32m---> 58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_name_or_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtokenizer_name_or_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtokenizer_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# No max_seq_length set. Try to infer from model\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_seq_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/mlingual/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:908\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    905\u001b[0m tokenizer_class_py, tokenizer_class_fast \u001b[38;5;241m=\u001b[39m TOKENIZER_MAPPING[\u001b[38;5;28mtype\u001b[39m(config)]\n\u001b[1;32m    907\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_fast \u001b[38;5;129;01mand\u001b[39;00m (use_fast \u001b[38;5;129;01mor\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 908\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class_fast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    910\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/mlingual/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2163\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2160\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2161\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2166\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2167\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2168\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2169\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2170\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2171\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2172\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2175\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlingual/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2378\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2376\u001b[0m     \u001b[38;5;66;03m# This is for slow so can be done before\u001b[39;00m\n\u001b[1;32m   2377\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(tokenizer_file, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m tokenizer_file_handle:\n\u001b[0;32m-> 2378\u001b[0m         tokenizer_file_handle \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer_file_handle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2379\u001b[0m         added_tokens \u001b[38;5;241m=\u001b[39m tokenizer_file_handle\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madded_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2380\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m serialized_tokens \u001b[38;5;129;01min\u001b[39;00m added_tokens:\n",
      "File \u001b[0;32m~/miniconda3/envs/mlingual/lib/python3.11/json/__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_hook\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_float\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_int\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_int\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_constant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_constant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlingual/lib/python3.11/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m~/miniconda3/envs/mlingual/lib/python3.11/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m~/miniconda3/envs/mlingual/lib/python3.11/json/decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;124;03ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;124;03mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m \n\u001b[1;32m    351\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from model_utils import FeatureSet, AggregationMethod, ModifiedSentenceTransformer\n",
    "# ----------------------------------------------------\n",
    "# Model declaration\n",
    "# ----------------------------------------------------\n",
    "model = model_attention = ModifiedSentenceTransformer(model_name=model_name, \n",
    "                                                      external_dim1=external_dim1, \n",
    "                                                      external_dim2=external_dim2, \n",
    "                                                      aggregation_method=AggregationMethod.LINEAR,\n",
    "                                                      feature_set=FeatureSet.SBERT_EXT1_EXT2)\n",
    "model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# Dataset & Dataloader declaration\n",
    "# ----------------------------------------------------\n",
    "# Get and Process the raw task dataset\n",
    "eval_facts, eval_posts, eval_mapping = get_raw_data_df(fact_path=os.path.join(eval_data_root, \"trial_fact_checks.csv\"), \n",
    "                                                posts_path=os.path.join(eval_data_root, \"trial_posts.csv\"),\n",
    "                                                post2fact_mapping_path=os.path.join(eval_data_root, \"trial_data_mapping.csv\"))\n",
    "eval_facts = process_facts_df(eval_facts)\n",
    "eval_posts = process_posts_df(eval_posts)\n",
    "\n",
    "# train_facts, train_posts, train_mapping = get_raw_data_df(fact_path=os.path.join(train_data_root, \"fact_checks.csv\"), \n",
    "#                                                     posts_path=os.path.join(train_data_root, \"posts.csv\"),\n",
    "#                                                     post2fact_mapping_path=os.path.join(train_data_root, \"fact_check_post_mapping.csv\"))   \n",
    "# train_facts = process_facts_df(train_facts)\n",
    "# train_posts = process_posts_df(train_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  2.38it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  3.38it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  3.25it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  3.24it/s]\n"
     ]
    }
   ],
   "source": [
    "result = evaluate(model=model, tokenizer=tokenizer, device=device, \n",
    "                  eval_posts=eval_posts, eval_facts=eval_facts, eval_mapping=eval_mapping, \n",
    "                  fact_orig_emb_pth = './../openai-op/eval_orig-fact.pkl', fact_eng_emb_pth = './../openai-op/eval_eng-fact.pkl', \n",
    "                  post_l1_emb_pth = './../openai-op/eval_l1-post.pkl', post_l2_emb_pth = './../openai-op/eval_l2-post.pkl',\n",
    "                  batch_size = 32, top_K = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of anchor-positive_example pair list for the training data\n",
    "eval_pair_dataset, eval_ext_emb_small, eval_ext_emb_large = create_train_dataset(df_facts = eval_facts, df_posts = eval_posts, df_mapping = eval_mapping, \n",
    "                                                                                            fact_orig_emb_pth = './../openai-op/eval_orig-fact.pkl', \n",
    "                                                                                            fact_eng_emb_pth = './../openai-op/eval_eng-fact.pkl', \n",
    "                                                                                            post_l1_emb_pth = './../openai-op/eval_l1-post.pkl', \n",
    "                                                                                            post_l2_emb_pth = './../openai-op/eval_l2-post.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_df = create_post_fact_df_with_ext_emb(df_facts= eval_facts, df_posts= eval_posts, df_mapping= eval_mapping, \n",
    "#                                             fact_orig_emb_pth = './../openai-op/eval_orig-fact.pkl', \n",
    "#                                             fact_eng_emb_pth = './../openai-op/eval_eng-fact.pkl', \n",
    "#                                             post_l1_emb_pth = './../openai-op/eval_l1-post.pkl', \n",
    "#                                             post_l2_emb_pth = './../openai-op/eval_l2-post.pkl',\n",
    "#                                             strategy=MergeStrategy.ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(eval_posts, eval_facts, fact_orig_emb_pth, fact_eng_emb_pth, post_l1_emb_pth, post_l2_emb_pth):\n",
    "\n",
    "    fact_ix2id_mapping = {ix: row['fact_check_id'] for ix, row in eval_facts.iterrows()}\n",
    "    get_fact_id = lambda x: fact_ix2id_mapping[x]\n",
    "    vfunc = np.vectorize(get_fact_id)\n",
    "\n",
    "    eval_dict = {'post_l1': TextType.POST_L1, 'post_l2': TextType.POST_L2, \n",
    "                'fact_orig': TextType.FACT_ORIG, 'fact_eng': TextType.FACT_ENG}\n",
    "    eval_data_inf = {}\n",
    "    eval_dataset_inf = {}\n",
    "    eval_dl_inf = {}\n",
    "    eval_model_repr = {}\n",
    "    temp_df = cc(eval_mapping)\n",
    "\n",
    "    for key, v in eval_dict.items():\n",
    "        text_list, small_emb_list, large_emb_list = create_inference_dataset(df_facts=eval_facts, df_posts=eval_posts, df_mapping=eval_mapping, \n",
    "                                                                            fact_orig_emb_pth = './../openai-op/eval_orig-fact.pkl', \n",
    "                                                                            fact_eng_emb_pth = './../openai-op/eval_eng-fact.pkl', \n",
    "                                                                            post_l1_emb_pth = './../openai-op/eval_l1-post.pkl', \n",
    "                                                                            post_l2_emb_pth = './../openai-op/eval_l2-post.pkl',\n",
    "                                                                            txt_type=v)\n",
    "        eval_data_inf[key] = (text_list, small_emb_list, large_emb_list)\n",
    "        eval_dataset_inf[key] = PairDatasetWithEmbeddings_for_inf(txt_list=text_list, external_embeddings1=small_emb_list, \n",
    "                                                                external_embeddings2=large_emb_list, tokenizer=tokenizer)\n",
    "        eval_dl_inf[key] = DataLoader(eval_dataset_inf[key], batch_size=32, shuffle=False)\n",
    "        eval_model_repr[key] = []\n",
    "\n",
    "        # Get model representation of the posts (l1, l2) and facts (orig, eng)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for ix, batch in tqdm(enumerate(eval_dl_inf[key])):\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                res = model(batch)\n",
    "                res = res.cpu().detach().numpy()\n",
    "                eval_model_repr[key].extend(res)\n",
    "            \n",
    "            if 'post' in key:\n",
    "                df = cc(eval_posts)\n",
    "                df[f'pred_{key}'] = eval_model_repr[key]\n",
    "                relevant_df = df[['post_id', f'pred_{key}']]\n",
    "                temp_df = temp_df.merge(relevant_df, how = 'left')\n",
    "\n",
    "            if 'fact' in key:\n",
    "                df = cc(eval_facts)\n",
    "                df[f'pred_{key}'] = eval_model_repr[key]\n",
    "                relevant_df = df[['fact_check_id', f'pred_{key}']]\n",
    "                temp_df = temp_df.merge(relevant_df, how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_ix2id_mapping = {ix: row['fact_check_id'] for ix, row in eval_facts.iterrows()}\n",
    "get_fact_id = lambda x: fact_ix2id_mapping[x]\n",
    "vfunc = np.vectorize(get_fact_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00,  5.18it/s]\n",
      "2it [00:00,  5.19it/s]\n",
      "2it [00:00,  4.89it/s]\n",
      "2it [00:00,  4.89it/s]\n"
     ]
    }
   ],
   "source": [
    "eval_dict = {'post_l1': TextType.POST_L1, 'post_l2': TextType.POST_L2, \n",
    "             'fact_orig': TextType.FACT_ORIG, 'fact_eng': TextType.FACT_ENG}\n",
    "\n",
    "eval_data_inf = {}\n",
    "eval_dataset_inf = {}\n",
    "eval_dl_inf = {}\n",
    "eval_model_repr = {}\n",
    "temp_df = cc(eval_mapping)\n",
    "\n",
    "for key, v in eval_dict.items():\n",
    "    text_list, small_emb_list, large_emb_list = create_inference_dataset(df_facts=eval_facts, df_posts=eval_posts, df_mapping=eval_mapping, \n",
    "                                                                         fact_orig_emb_pth = './../openai-op/eval_orig-fact.pkl', \n",
    "                                                                         fact_eng_emb_pth = './../openai-op/eval_eng-fact.pkl', \n",
    "                                                                         post_l1_emb_pth = './../openai-op/eval_l1-post.pkl', \n",
    "                                                                         post_l2_emb_pth = './../openai-op/eval_l2-post.pkl',\n",
    "                                                                         txt_type=v)\n",
    "    eval_data_inf[key] = (text_list, small_emb_list, large_emb_list)\n",
    "    eval_dataset_inf[key] = PairDatasetWithEmbeddings_for_inf(txt_list=text_list, external_embeddings1=small_emb_list, \n",
    "                                                            external_embeddings2=large_emb_list, tokenizer=tokenizer)\n",
    "    eval_dl_inf[key] = DataLoader(eval_dataset_inf[key], batch_size=32, shuffle=False)\n",
    "    eval_model_repr[key] = []\n",
    "\n",
    "    # Get model representation of the posts (l1, l2) and facts (orig, eng)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for ix, batch in tqdm(enumerate(eval_dl_inf[key])):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            res = model(batch)\n",
    "            res = res.cpu().detach().numpy()\n",
    "            eval_model_repr[key].extend(res)\n",
    "          \n",
    "        if 'post' in key:\n",
    "            df = cc(eval_posts)\n",
    "            df[f'pred_{key}'] = eval_model_repr[key]\n",
    "            relevant_df = df[['post_id', f'pred_{key}']]\n",
    "            temp_df = temp_df.merge(relevant_df, how = 'left')\n",
    "\n",
    "        if 'fact' in key:\n",
    "            df = cc(eval_facts)\n",
    "            df[f'pred_{key}'] = eval_model_repr[key]\n",
    "            relevant_df = df[['fact_check_id', f'pred_{key}']]\n",
    "            temp_df = temp_df.merge(relevant_df, how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post_l1 <-> fact_eng\n",
      "[0.62, 0.8, 0.82, 0.84, 0.84]\n",
      "\n",
      "post_l1 <-> fact_orig\n",
      "[0.64, 0.74, 0.84, 0.84, 0.86]\n",
      "\n",
      "post_l2 <-> fact_eng\n",
      "[0.7, 0.82, 0.86, 0.9, 0.92]\n",
      "\n",
      "post_l2 <-> fact_orig\n",
      "[0.68, 0.76, 0.84, 0.84, 0.86]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "metric_keys = [('post_l1', 'fact_eng'), ('post_l1', 'fact_orig'), ('post_l2', 'fact_eng'), ('post_l2', 'fact_orig')]\n",
    "\n",
    "for post_key, fact_key in metric_keys:\n",
    "    print(f'{post_key} <-> {fact_key}')\n",
    "    pred = cosine_similarity(temp_df[f'pred_{post_key}'].tolist(), temp_df[f'pred_{fact_key}'].tolist())\n",
    "\n",
    "    res = []\n",
    "    for k in range(1, 6):\n",
    "        preds = (-pred).argsort()[:, :k]\n",
    "        res.append(np.mean([(i in pred) for  i, pred in enumerate(preds)]))\n",
    "    print(f'{res}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_acc(pred_fact_id_matrix, true_fact_id_list, top_K):\n",
    "    result = []\n",
    "\n",
    "    for true_id, pred_ids in zip(true_fact_id_list, pred_fact_id_matrix):\n",
    "        correct_pred = [(true_id in pred_ids[:k]) for k in range(1, (top_K + 1))]\n",
    "        result.append(correct_pred)\n",
    "        \n",
    "    result = np.array(result).mean(axis = 0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post_l1 <-> fact_eng\n",
      "[0.62 0.8  0.82 0.84 0.84]\n",
      "post_l1 <-> fact_orig\n",
      "[0.64 0.74 0.84 0.84 0.86]\n",
      "post_l2 <-> fact_eng\n",
      "[0.7  0.82 0.86 0.9  0.92]\n",
      "post_l2 <-> fact_orig\n",
      "[0.68 0.76 0.84 0.84 0.86]\n"
     ]
    }
   ],
   "source": [
    "eval_results = {}\n",
    "metric_keys = [('post_l1', 'fact_eng'), ('post_l1', 'fact_orig'), ('post_l2', 'fact_eng'), ('post_l2', 'fact_orig')]\n",
    "\n",
    "for post_key, fact_key in metric_keys:\n",
    "    print(f'{post_key} <-> {fact_key}')\n",
    "\n",
    "    fact_repr_list = eval_model_repr[fact_key]    \n",
    "    post_repr_list = temp_df[f'pred_{post_key}'].tolist()\n",
    "    true_fact_id_list = temp_df[f'fact_check_id'].tolist()\n",
    "\n",
    "    pred = cosine_similarity(post_repr_list, fact_repr_list)\n",
    "    pred_fact_ids = vfunc((-pred).argsort())\n",
    "    \n",
    "    res = get_top_k_acc(pred_fact_ids, true_fact_id_list, top_K = 5)\n",
    "    eval_results[f'{post_key}_{fact_key}'] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = cosine_similarity(temp_df[f'pred_{post_key}'].tolist(), eval_model_repr[fact_key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_ix2id_mapping = {ix: row['fact_check_id'] for ix, row in eval_facts.iterrows()}\n",
    "get_post_id = lambda x: fact_ix2id_mapping[x]\n",
    "vfunc = np.vectorize(get_post_id)\n",
    "\n",
    "preds = vfunc((-pred).argsort())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[27, 28, 33, ..., 46, 14,  1],\n",
       "       [19, 11, 16, ...,  1, 26, 27],\n",
       "       [ 9,  5, 37, ..., 48, 29, 45],\n",
       "       ...,\n",
       "       [26, 18, 41, ...,  2,  9,  1],\n",
       "       [ 1, 41,  9, ..., 29, 27, 30],\n",
       "       [ 6, 25,  4, ..., 22, 45, 21]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_fact_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct = []\n",
    "for true, pred in zip(temp_df['fact_check_id'], preds):\n",
    "    correct.append(true in pred[:5])\n",
    "np.mean(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlingual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
