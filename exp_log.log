Running experiment with feature_type=1, agg_type=1 (LINEAR), batch_size=16, lr=1e-4
2024-08-20 05:49:57,246 - INFO - Use pytorch device_name: cuda
2024-08-20 05:49:57,248 - INFO - Load pretrained SentenceTransformer: sentence-transformers/use-cmlm-multilingual
Some weights of the model checkpoint at sentence-transformers/use-cmlm-multilingual were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2024-08-20 05:49:58,925 - INFO - Use pytorch device_name: cuda
2024-08-20 05:49:58,926 - INFO - Load pretrained SentenceTransformer: sentence-transformers/use-cmlm-multilingual
Some weights of the model checkpoint at sentence-transformers/use-cmlm-multilingual were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Seed set to 447
{'feature_type': <FeatureSet.SBERT_ONLY: 1>, 'agg_type': <AggregationMethod.LINEAR: 1>, 'nheads': 0, 'model_name': 'sentence-transformers/use-cmlm-multilingual', 'external_dim1': 1536, 'external_dim2': 3072, 'device': 'cuda:0', 'seed': 447, 'train_fact_path': './sample_data/trial_fact_checks.csv', 'train_post_path': './sample_data/trial_posts.csv', 'train_post2fact_mapping': './sample_data/trial_data_mapping.csv', 'train_fact_orig_emb_path': './openai-op/eval_orig-fact.pkl', 'train_fact_eng_emb_path': './openai-op/eval_eng-fact.pkl', 'train_post_l1_emb_path': './openai-op/eval_l1-post.pkl', 'train_post_l2_emb_path': './openai-op/eval_l2-post.pkl', 'eval_fact_path': './sample_data/trial_fact_checks.csv', 'eval_post_path': './sample_data/trial_posts.csv', 'eval_post2fact_mapping': './sample_data/trial_data_mapping.csv', 'eval_fact_orig_emb_path': './openai-op/eval_orig-fact.pkl', 'eval_fact_eng_emb_path': './openai-op/eval_eng-fact.pkl', 'eval_post_l1_emb_path': './openai-op/eval_l1-post.pkl', 'eval_post_l2_emb_path': './openai-op/eval_l2-post.pkl', 'loss_scale': 20, 'train_batch_size': 16, 'accumulation_steps': 1, 'eval_batch_size': 16, 'max_seq_len': 312, 'n_epochs': 2, 'wt_decay': 1e-08, 'lr': 0.0001, 'K': 5, 'log_path': './logs/447/f-SBERT_ONLY_a-LINEAR_b-16_nh-0.log', 'checkpoint_dir': './checkpoints/', 'model_checkpointing': False, 'load_checkpoint': False, 'checkpoint_path': './checkpoints/checkpoint_epoch_1.pth', 'loss_log_step': -1}
0it [00:00, ?it/s]50it [00:00, 17787.55it/s]
/home/ubuntu/miniconda3/envs/mlingual/lib/python3.11/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
2024-08-20 05:50:02,570 - INFO - Evaluation result at epoch 0:
	   post_l1_fact_eng  post_l1_fact_orig  post_l2_fact_eng  post_l2_fact_orig
	0              0.66               0.60              0.70               0.66
	1              0.78               0.70              0.82               0.72
	2              0.80               0.78              0.84               0.78
	3              0.82               0.82              0.86               0.82
	4              0.82               0.84              0.86               0.82
  0%|          | 0/9 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/9 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/9 [00:00<?, ?it/s, loss=0.825]Epoch 0:  11%|█         | 1/9 [00:00<00:03,  2.15it/s, loss=0.825]Epoch 0:  11%|█         | 1/9 [00:00<00:03,  2.15it/s, loss=0.825]Epoch 0:  11%|█         | 1/9 [00:00<00:03,  2.15it/s, loss=1.34] Epoch 0:  22%|██▏       | 2/9 [00:00<00:02,  3.33it/s, loss=1.34]Epoch 0:  22%|██▏       | 2/9 [00:00<00:02,  3.33it/s, loss=1.34]Epoch 0:  22%|██▏       | 2/9 [00:00<00:02,  3.33it/s, loss=1.02]Epoch 0:  33%|███▎      | 3/9 [00:00<00:01,  4.07it/s, loss=1.02]Epoch 0:  33%|███▎      | 3/9 [00:00<00:01,  4.07it/s, loss=1.02]Epoch 0:  33%|███▎      | 3/9 [00:00<00:01,  4.07it/s, loss=0.312]Epoch 0:  44%|████▍     | 4/9 [00:00<00:01,  4.67it/s, loss=0.312]Epoch 0:  44%|████▍     | 4/9 [00:01<00:01,  4.67it/s, loss=0.312]Epoch 0:  44%|████▍     | 4/9 [00:01<00:01,  4.67it/s, loss=0.637]Epoch 0:  56%|█████▌    | 5/9 [00:01<00:00,  5.11it/s, loss=0.637]Epoch 0:  56%|█████▌    | 5/9 [00:01<00:00,  5.11it/s, loss=0.637]Epoch 0:  56%|█████▌    | 5/9 [00:01<00:00,  5.11it/s, loss=0.342]Epoch 0:  67%|██████▋   | 6/9 [00:01<00:00,  5.32it/s, loss=0.342]Epoch 0:  67%|██████▋   | 6/9 [00:01<00:00,  5.32it/s, loss=0.342]Epoch 0:  67%|██████▋   | 6/9 [00:01<00:00,  5.32it/s, loss=0.499]Epoch 0:  78%|███████▊  | 7/9 [00:01<00:00,  5.46it/s, loss=0.499]Epoch 0:  78%|███████▊  | 7/9 [00:01<00:00,  5.46it/s, loss=0.499]Epoch 0:  78%|███████▊  | 7/9 [00:01<00:00,  5.46it/s, loss=0.553]Epoch 0:  89%|████████▉ | 8/9 [00:01<00:00,  5.51it/s, loss=0.553]Epoch 0:  89%|████████▉ | 8/9 [00:01<00:00,  5.51it/s, loss=0.553]Epoch 0:  89%|████████▉ | 8/9 [00:01<00:00,  5.51it/s, loss=8.47e-5]Epoch 0: 100%|██████████| 9/9 [00:01<00:00,  6.02it/s, loss=8.47e-5]Epoch 0: 100%|██████████| 9/9 [00:01<00:00,  4.81it/s, loss=8.47e-5]
2024-08-20 05:50:04,443 - INFO - Epoch 0: Last Batch Loss = 8.469403110211715e-05
2024-08-20 05:50:05,823 - INFO - Evaluation result at epoch 1:
	   post_l1_fact_eng  post_l1_fact_orig  post_l2_fact_eng  post_l2_fact_orig
	0              0.94               0.94              0.94               0.94
	1              0.96               0.98              0.98               0.98
	2              1.00               1.00              1.00               1.00
	3              1.00               1.00              1.00               1.00
	4              1.00               1.00              1.00               1.00
  0%|          | 0/9 [00:00<?, ?it/s]Epoch 1:   0%|          | 0/9 [00:00<?, ?it/s]Epoch 1:   0%|          | 0/9 [00:00<?, ?it/s, loss=0.255]Epoch 1:  11%|█         | 1/9 [00:00<00:03,  2.44it/s, loss=0.255]Epoch 1:  11%|█         | 1/9 [00:00<00:03,  2.44it/s, loss=0.255]Epoch 1:  11%|█         | 1/9 [00:00<00:03,  2.44it/s, loss=0.338]Epoch 1:  22%|██▏       | 2/9 [00:00<00:01,  3.63it/s, loss=0.338]Epoch 1:  22%|██▏       | 2/9 [00:00<00:01,  3.63it/s, loss=0.338]Epoch 1:  22%|██▏       | 2/9 [00:00<00:01,  3.63it/s, loss=0.336]Epoch 1:  33%|███▎      | 3/9 [00:00<00:01,  4.31it/s, loss=0.336]Epoch 1:  33%|███▎      | 3/9 [00:00<00:01,  4.31it/s, loss=0.336]Epoch 1:  33%|███▎      | 3/9 [00:00<00:01,  4.31it/s, loss=0.189]Epoch 1:  44%|████▍     | 4/9 [00:00<00:01,  4.69it/s, loss=0.189]Epoch 1:  44%|████▍     | 4/9 [00:01<00:01,  4.69it/s, loss=0.189]Epoch 1:  44%|████▍     | 4/9 [00:01<00:01,  4.69it/s, loss=0.142]Epoch 1:  56%|█████▌    | 5/9 [00:01<00:00,  5.04it/s, loss=0.142]Epoch 1:  56%|█████▌    | 5/9 [00:01<00:00,  5.04it/s, loss=0.142]Epoch 1:  56%|█████▌    | 5/9 [00:01<00:00,  5.04it/s, loss=0.2]  Epoch 1:  67%|██████▋   | 6/9 [00:01<00:00,  5.24it/s, loss=0.2]Epoch 1:  67%|██████▋   | 6/9 [00:01<00:00,  5.24it/s, loss=0.2]Epoch 1:  67%|██████▋   | 6/9 [00:01<00:00,  5.24it/s, loss=0.128]Epoch 1:  78%|███████▊  | 7/9 [00:01<00:00,  5.44it/s, loss=0.128]Epoch 1:  78%|███████▊  | 7/9 [00:01<00:00,  5.44it/s, loss=0.128]Epoch 1:  78%|███████▊  | 7/9 [00:01<00:00,  5.44it/s, loss=0.186]Epoch 1:  89%|████████▉ | 8/9 [00:01<00:00,  5.68it/s, loss=0.186]Epoch 1:  89%|████████▉ | 8/9 [00:01<00:00,  5.68it/s, loss=0.186]Epoch 1:  89%|████████▉ | 8/9 [00:01<00:00,  5.68it/s, loss=0.00052]Epoch 1: 100%|██████████| 9/9 [00:01<00:00,  6.20it/s, loss=0.00052]Epoch 1: 100%|██████████| 9/9 [00:01<00:00,  4.96it/s, loss=0.00052]
2024-08-20 05:50:07,640 - INFO - Epoch 1: Last Batch Loss = 0.0005203819018788636
2024-08-20 05:50:09,052 - INFO - Evaluation result at epoch 2:
	   post_l1_fact_eng  post_l1_fact_orig  post_l2_fact_eng  post_l2_fact_orig
	0              0.94               0.94              0.94               0.94
	1              0.98               0.98              0.98               0.98
	2              1.00               1.00              1.00               1.00
	3              1.00               1.00              1.00               1.00
	4              1.00               1.00              1.00               1.00
Running experiment with feature_type=1, agg_type=2 (ATTENTION), nheads=4, batch_size=16, lr=1e-4
2024-08-20 05:50:14,284 - INFO - Use pytorch device_name: cuda
2024-08-20 05:50:14,284 - INFO - Load pretrained SentenceTransformer: sentence-transformers/use-cmlm-multilingual
Some weights of the model checkpoint at sentence-transformers/use-cmlm-multilingual were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2024-08-20 05:50:15,952 - INFO - Use pytorch device_name: cuda
2024-08-20 05:50:15,952 - INFO - Load pretrained SentenceTransformer: sentence-transformers/use-cmlm-multilingual
Some weights of the model checkpoint at sentence-transformers/use-cmlm-multilingual were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Seed set to 447
{'feature_type': <FeatureSet.SBERT_ONLY: 1>, 'agg_type': <AggregationMethod.ATTENTION: 2>, 'nheads': 4, 'model_name': 'sentence-transformers/use-cmlm-multilingual', 'external_dim1': 1536, 'external_dim2': 3072, 'device': 'cuda:0', 'seed': 447, 'train_fact_path': './sample_data/trial_fact_checks.csv', 'train_post_path': './sample_data/trial_posts.csv', 'train_post2fact_mapping': './sample_data/trial_data_mapping.csv', 'train_fact_orig_emb_path': './openai-op/eval_orig-fact.pkl', 'train_fact_eng_emb_path': './openai-op/eval_eng-fact.pkl', 'train_post_l1_emb_path': './openai-op/eval_l1-post.pkl', 'train_post_l2_emb_path': './openai-op/eval_l2-post.pkl', 'eval_fact_path': './sample_data/trial_fact_checks.csv', 'eval_post_path': './sample_data/trial_posts.csv', 'eval_post2fact_mapping': './sample_data/trial_data_mapping.csv', 'eval_fact_orig_emb_path': './openai-op/eval_orig-fact.pkl', 'eval_fact_eng_emb_path': './openai-op/eval_eng-fact.pkl', 'eval_post_l1_emb_path': './openai-op/eval_l1-post.pkl', 'eval_post_l2_emb_path': './openai-op/eval_l2-post.pkl', 'loss_scale': 20, 'train_batch_size': 16, 'accumulation_steps': 1, 'eval_batch_size': 16, 'max_seq_len': 312, 'n_epochs': 2, 'wt_decay': 1e-08, 'lr': 0.0001, 'K': 5, 'log_path': './logs/447/f-SBERT_ONLY_a-ATTENTION_b-16_nh-4.log', 'checkpoint_dir': './checkpoints/', 'model_checkpointing': False, 'load_checkpoint': False, 'checkpoint_path': './checkpoints/checkpoint_epoch_1.pth', 'loss_log_step': -1}
0it [00:00, ?it/s]50it [00:00, 17546.45it/s]
/home/ubuntu/miniconda3/envs/mlingual/lib/python3.11/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
2024-08-20 05:50:19,612 - INFO - Evaluation result at epoch 0:
	   post_l1_fact_eng  post_l1_fact_orig  post_l2_fact_eng  post_l2_fact_orig
	0              0.66               0.60              0.70               0.66
	1              0.78               0.70              0.82               0.72
	2              0.80               0.78              0.84               0.78
	3              0.82               0.82              0.86               0.82
	4              0.82               0.84              0.86               0.82
  0%|          | 0/9 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/9 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/9 [00:00<?, ?it/s, loss=0.948]Epoch 0:  11%|█         | 1/9 [00:00<00:03,  2.01it/s, loss=0.948]Epoch 0:  11%|█         | 1/9 [00:00<00:03,  2.01it/s, loss=0.948]Epoch 0:  11%|█         | 1/9 [00:00<00:03,  2.01it/s, loss=1.3]  Epoch 0:  22%|██▏       | 2/9 [00:00<00:02,  3.24it/s, loss=1.3]Epoch 0:  22%|██▏       | 2/9 [00:00<00:02,  3.24it/s, loss=1.3]Epoch 0:  22%|██▏       | 2/9 [00:00<00:02,  3.24it/s, loss=0.548]Epoch 0:  33%|███▎      | 3/9 [00:00<00:01,  4.04it/s, loss=0.548]Epoch 0:  33%|███▎      | 3/9 [00:01<00:01,  4.04it/s, loss=0.548]Epoch 0:  33%|███▎      | 3/9 [00:01<00:01,  4.04it/s, loss=0.482]Epoch 0:  44%|████▍     | 4/9 [00:01<00:01,  4.67it/s, loss=0.482]Epoch 0:  44%|████▍     | 4/9 [00:01<00:01,  4.67it/s, loss=0.482]Epoch 0:  44%|████▍     | 4/9 [00:01<00:01,  4.67it/s, loss=0.331]Epoch 0:  56%|█████▌    | 5/9 [00:01<00:00,  4.95it/s, loss=0.331]Epoch 0:  56%|█████▌    | 5/9 [00:01<00:00,  4.95it/s, loss=0.331]Epoch 0:  56%|█████▌    | 5/9 [00:01<00:00,  4.95it/s, loss=0.498]Epoch 0:  67%|██████▋   | 6/9 [00:01<00:00,  5.15it/s, loss=0.498]Epoch 0:  67%|██████▋   | 6/9 [00:01<00:00,  5.15it/s, loss=0.498]Epoch 0:  67%|██████▋   | 6/9 [00:01<00:00,  5.15it/s, loss=0.392]Epoch 0:  78%|███████▊  | 7/9 [00:01<00:00,  5.33it/s, loss=0.392]Epoch 0:  78%|███████▊  | 7/9 [00:01<00:00,  5.33it/s, loss=0.392]Epoch 0:  78%|███████▊  | 7/9 [00:01<00:00,  5.33it/s, loss=0.544]Epoch 0:  89%|████████▉ | 8/9 [00:01<00:00,  5.40it/s, loss=0.544]Epoch 0:  89%|████████▉ | 8/9 [00:01<00:00,  5.40it/s, loss=0.544]Epoch 0:  89%|████████▉ | 8/9 [00:01<00:00,  5.40it/s, loss=0.00102]Epoch 0: 100%|██████████| 9/9 [00:01<00:00,  5.91it/s, loss=0.00102]Epoch 0: 100%|██████████| 9/9 [00:01<00:00,  4.70it/s, loss=0.00102]
2024-08-20 05:50:21,527 - INFO - Epoch 0: Last Batch Loss = 0.0010237032547593117
2024-08-20 05:50:22,910 - INFO - Evaluation result at epoch 1:
	   post_l1_fact_eng  post_l1_fact_orig  post_l2_fact_eng  post_l2_fact_orig
	0              0.90               0.90              0.92               0.92
	1              0.96               0.96              0.96               0.96
	2              1.00               1.00              1.00               1.00
	3              1.00               1.00              1.00               1.00
	4              1.00               1.00              1.00               1.00
  0%|          | 0/9 [00:00<?, ?it/s]Epoch 1:   0%|          | 0/9 [00:00<?, ?it/s]Epoch 1:   0%|          | 0/9 [00:00<?, ?it/s, loss=0.25]Epoch 1:  11%|█         | 1/9 [00:00<00:03,  2.59it/s, loss=0.25]Epoch 1:  11%|█         | 1/9 [00:00<00:03,  2.59it/s, loss=0.25]Epoch 1:  11%|█         | 1/9 [00:00<00:03,  2.59it/s, loss=0.236]Epoch 1:  22%|██▏       | 2/9 [00:00<00:01,  3.84it/s, loss=0.236]Epoch 1:  22%|██▏       | 2/9 [00:00<00:01,  3.84it/s, loss=0.236]Epoch 1:  22%|██▏       | 2/9 [00:00<00:01,  3.84it/s, loss=0.212]Epoch 1:  33%|███▎      | 3/9 [00:00<00:01,  4.60it/s, loss=0.212]Epoch 1:  33%|███▎      | 3/9 [00:00<00:01,  4.60it/s, loss=0.212]Epoch 1:  33%|███▎      | 3/9 [00:00<00:01,  4.60it/s, loss=0.107]Epoch 1:  44%|████▍     | 4/9 [00:00<00:01,  4.85it/s, loss=0.107]Epoch 1:  44%|████▍     | 4/9 [00:01<00:01,  4.85it/s, loss=0.107]Epoch 1:  44%|████▍     | 4/9 [00:01<00:01,  4.85it/s, loss=0.175]Epoch 1:  56%|█████▌    | 5/9 [00:01<00:00,  5.20it/s, loss=0.175]Epoch 1:  56%|█████▌    | 5/9 [00:01<00:00,  5.20it/s, loss=0.175]Epoch 1:  56%|█████▌    | 5/9 [00:01<00:00,  5.20it/s, loss=0.253]Epoch 1:  67%|██████▋   | 6/9 [00:01<00:00,  5.37it/s, loss=0.253]Epoch 1:  67%|██████▋   | 6/9 [00:01<00:00,  5.37it/s, loss=0.253]Epoch 1:  67%|██████▋   | 6/9 [00:01<00:00,  5.37it/s, loss=0.409]Epoch 1:  78%|███████▊  | 7/9 [00:01<00:00,  5.51it/s, loss=0.409]Epoch 1:  78%|███████▊  | 7/9 [00:01<00:00,  5.51it/s, loss=0.409]Epoch 1:  78%|███████▊  | 7/9 [00:01<00:00,  5.51it/s, loss=0.21] Epoch 1:  89%|████████▉ | 8/9 [00:01<00:00,  5.51it/s, loss=0.21]Epoch 1:  89%|████████▉ | 8/9 [00:01<00:00,  5.51it/s, loss=0.21]Epoch 1:  89%|████████▉ | 8/9 [00:01<00:00,  5.51it/s, loss=8.57e-5]Epoch 1: 100%|██████████| 9/9 [00:01<00:00,  6.05it/s, loss=8.57e-5]Epoch 1: 100%|██████████| 9/9 [00:01<00:00,  5.02it/s, loss=8.57e-5]
2024-08-20 05:50:24,706 - INFO - Epoch 1: Last Batch Loss = 8.570760837756097e-05
2024-08-20 05:50:26,081 - INFO - Evaluation result at epoch 2:
	   post_l1_fact_eng  post_l1_fact_orig  post_l2_fact_eng  post_l2_fact_orig
	0              0.92               0.92              0.92               0.92
	1              0.96               0.96              0.96               0.96
	2              1.00               1.00              1.00               1.00
	3              1.00               1.00              1.00               1.00
	4              1.00               1.00              1.00               1.00
Running experiment with feature_type=1, agg_type=2 (ATTENTION), nheads=8, batch_size=16, lr=1e-4
2024-08-20 05:50:31,254 - INFO - Use pytorch device_name: cuda
2024-08-20 05:50:31,254 - INFO - Load pretrained SentenceTransformer: sentence-transformers/use-cmlm-multilingual
Some weights of the model checkpoint at sentence-transformers/use-cmlm-multilingual were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2024-08-20 05:50:32,831 - INFO - Use pytorch device_name: cuda
2024-08-20 05:50:32,832 - INFO - Load pretrained SentenceTransformer: sentence-transformers/use-cmlm-multilingual
Some weights of the model checkpoint at sentence-transformers/use-cmlm-multilingual were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Seed set to 447
{'feature_type': <FeatureSet.SBERT_ONLY: 1>, 'agg_type': <AggregationMethod.ATTENTION: 2>, 'nheads': 8, 'model_name': 'sentence-transformers/use-cmlm-multilingual', 'external_dim1': 1536, 'external_dim2': 3072, 'device': 'cuda:0', 'seed': 447, 'train_fact_path': './sample_data/trial_fact_checks.csv', 'train_post_path': './sample_data/trial_posts.csv', 'train_post2fact_mapping': './sample_data/trial_data_mapping.csv', 'train_fact_orig_emb_path': './openai-op/eval_orig-fact.pkl', 'train_fact_eng_emb_path': './openai-op/eval_eng-fact.pkl', 'train_post_l1_emb_path': './openai-op/eval_l1-post.pkl', 'train_post_l2_emb_path': './openai-op/eval_l2-post.pkl', 'eval_fact_path': './sample_data/trial_fact_checks.csv', 'eval_post_path': './sample_data/trial_posts.csv', 'eval_post2fact_mapping': './sample_data/trial_data_mapping.csv', 'eval_fact_orig_emb_path': './openai-op/eval_orig-fact.pkl', 'eval_fact_eng_emb_path': './openai-op/eval_eng-fact.pkl', 'eval_post_l1_emb_path': './openai-op/eval_l1-post.pkl', 'eval_post_l2_emb_path': './openai-op/eval_l2-post.pkl', 'loss_scale': 20, 'train_batch_size': 16, 'accumulation_steps': 1, 'eval_batch_size': 16, 'max_seq_len': 312, 'n_epochs': 2, 'wt_decay': 1e-08, 'lr': 0.0001, 'K': 5, 'log_path': './logs/447/f-SBERT_ONLY_a-ATTENTION_b-16_nh-8.log', 'checkpoint_dir': './checkpoints/', 'model_checkpointing': False, 'load_checkpoint': False, 'checkpoint_path': './checkpoints/checkpoint_epoch_1.pth', 'loss_log_step': -1}
0it [00:00, ?it/s]50it [00:00, 17530.32it/s]
/home/ubuntu/miniconda3/envs/mlingual/lib/python3.11/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
2024-08-20 05:50:36,464 - INFO - Evaluation result at epoch 0:
	   post_l1_fact_eng  post_l1_fact_orig  post_l2_fact_eng  post_l2_fact_orig
	0              0.66               0.60              0.70               0.66
	1              0.78               0.70              0.82               0.72
	2              0.80               0.78              0.84               0.78
	3              0.82               0.82              0.86               0.82
	4              0.82               0.84              0.86               0.82
  0%|          | 0/9 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/9 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/9 [00:00<?, ?it/s, loss=0.948]Epoch 0:  11%|█         | 1/9 [00:00<00:04,  1.98it/s, loss=0.948]Epoch 0:  11%|█         | 1/9 [00:00<00:04,  1.98it/s, loss=0.948]Epoch 0:  11%|█         | 1/9 [00:00<00:04,  1.98it/s, loss=1.3]  Epoch 0:  22%|██▏       | 2/9 [00:00<00:02,  3.21it/s, loss=1.3]Epoch 0:  22%|██▏       | 2/9 [00:00<00:02,  3.21it/s, loss=1.3]Epoch 0:  22%|██▏       | 2/9 [00:00<00:02,  3.21it/s, loss=0.548]Epoch 0:  33%|███▎      | 3/9 [00:00<00:01,  4.04it/s, loss=0.548]Epoch 0:  33%|███▎      | 3/9 [00:01<00:01,  4.04it/s, loss=0.548]Epoch 0:  33%|███▎      | 3/9 [00:01<00:01,  4.04it/s, loss=0.482]Epoch 0:  44%|████▍     | 4/9 [00:01<00:01,  4.70it/s, loss=0.482]Epoch 0:  44%|████▍     | 4/9 [00:01<00:01,  4.70it/s, loss=0.482]Epoch 0:  44%|████▍     | 4/9 [00:01<00:01,  4.70it/s, loss=0.331]Epoch 0:  56%|█████▌    | 5/9 [00:01<00:00,  4.96it/s, loss=0.331]Epoch 0:  56%|█████▌    | 5/9 [00:01<00:00,  4.96it/s, loss=0.331]Epoch 0:  56%|█████▌    | 5/9 [00:01<00:00,  4.96it/s, loss=0.498]Epoch 0:  67%|██████▋   | 6/9 [00:01<00:00,  5.16it/s, loss=0.498]Epoch 0:  67%|██████▋   | 6/9 [00:01<00:00,  5.16it/s, loss=0.498]Epoch 0:  67%|██████▋   | 6/9 [00:01<00:00,  5.16it/s, loss=0.392]Epoch 0:  78%|███████▊  | 7/9 [00:01<00:00,  5.36it/s, loss=0.392]Epoch 0:  78%|███████▊  | 7/9 [00:01<00:00,  5.36it/s, loss=0.392]Epoch 0:  78%|███████▊  | 7/9 [00:01<00:00,  5.36it/s, loss=0.544]Epoch 0:  89%|████████▉ | 8/9 [00:01<00:00,  5.44it/s, loss=0.544]Epoch 0:  89%|████████▉ | 8/9 [00:01<00:00,  5.44it/s, loss=0.544]Epoch 0:  89%|████████▉ | 8/9 [00:01<00:00,  5.44it/s, loss=0.00102]Epoch 0: 100%|██████████| 9/9 [00:01<00:00,  5.94it/s, loss=0.00102]Epoch 0: 100%|██████████| 9/9 [00:01<00:00,  4.72it/s, loss=0.00102]
2024-08-20 05:50:38,372 - INFO - Epoch 0: Last Batch Loss = 0.0010237032547593117
2024-08-20 05:50:39,758 - INFO - Evaluation result at epoch 1:
	   post_l1_fact_eng  post_l1_fact_orig  post_l2_fact_eng  post_l2_fact_orig
	0              0.90               0.90              0.92               0.92
	1              0.96               0.96              0.96               0.96
	2              1.00               1.00              1.00               1.00
	3              1.00               1.00              1.00               1.00
	4              1.00               1.00              1.00               1.00
  0%|          | 0/9 [00:00<?, ?it/s]Epoch 1:   0%|          | 0/9 [00:00<?, ?it/s]Epoch 1:   0%|          | 0/9 [00:00<?, ?it/s, loss=0.25]Epoch 1:  11%|█         | 1/9 [00:00<00:03,  2.57it/s, loss=0.25]Epoch 1:  11%|█         | 1/9 [00:00<00:03,  2.57it/s, loss=0.25]Epoch 1:  11%|█         | 1/9 [00:00<00:03,  2.57it/s, loss=0.236]Epoch 1:  22%|██▏       | 2/9 [00:00<00:01,  3.90it/s, loss=0.236]Epoch 1:  22%|██▏       | 2/9 [00:00<00:01,  3.90it/s, loss=0.236]Epoch 1:  22%|██▏       | 2/9 [00:00<00:01,  3.90it/s, loss=0.212]Epoch 1:  33%|███▎      | 3/9 [00:00<00:01,  4.61it/s, loss=0.212]Epoch 1:  33%|███▎      | 3/9 [00:00<00:01,  4.61it/s, loss=0.212]Epoch 1:  33%|███▎      | 3/9 [00:00<00:01,  4.61it/s, loss=0.107]Epoch 1:  44%|████▍     | 4/9 [00:00<00:01,  4.83it/s, loss=0.107]Epoch 1:  44%|████▍     | 4/9 [00:01<00:01,  4.83it/s, loss=0.107]Epoch 1:  44%|████▍     | 4/9 [00:01<00:01,  4.83it/s, loss=0.175]Epoch 1:  56%|█████▌    | 5/9 [00:01<00:00,  5.17it/s, loss=0.175]Epoch 1:  56%|█████▌    | 5/9 [00:01<00:00,  5.17it/s, loss=0.175]Epoch 1:  56%|█████▌    | 5/9 [00:01<00:00,  5.17it/s, loss=0.253]Epoch 1:  67%|██████▋   | 6/9 [00:01<00:00,  5.34it/s, loss=0.253]Epoch 1:  67%|██████▋   | 6/9 [00:01<00:00,  5.34it/s, loss=0.253]Epoch 1:  67%|██████▋   | 6/9 [00:01<00:00,  5.34it/s, loss=0.409]Epoch 1:  78%|███████▊  | 7/9 [00:01<00:00,  5.49it/s, loss=0.409]Epoch 1:  78%|███████▊  | 7/9 [00:01<00:00,  5.49it/s, loss=0.409]Epoch 1:  78%|███████▊  | 7/9 [00:01<00:00,  5.49it/s, loss=0.21] Epoch 1:  89%|████████▉ | 8/9 [00:01<00:00,  5.48it/s, loss=0.21]Epoch 1:  89%|████████▉ | 8/9 [00:01<00:00,  5.48it/s, loss=0.21]Epoch 1:  89%|████████▉ | 8/9 [00:01<00:00,  5.48it/s, loss=8.57e-5]Epoch 1: 100%|██████████| 9/9 [00:01<00:00,  6.01it/s, loss=8.57e-5]Epoch 1: 100%|██████████| 9/9 [00:01<00:00,  5.00it/s, loss=8.57e-5]
2024-08-20 05:50:41,561 - INFO - Epoch 1: Last Batch Loss = 8.570760837756097e-05
2024-08-20 05:50:42,948 - INFO - Evaluation result at epoch 2:
	   post_l1_fact_eng  post_l1_fact_orig  post_l2_fact_eng  post_l2_fact_orig
	0              0.92               0.92              0.92               0.92
	1              0.96               0.96              0.96               0.96
	2              1.00               1.00              1.00               1.00
	3              1.00               1.00              1.00               1.00
	4              1.00               1.00              1.00               1.00
Running experiment with feature_type=2, agg_type=1 (LINEAR), batch_size=16, lr=1e-4
2024-08-20 05:50:48,170 - INFO - Use pytorch device_name: cuda
2024-08-20 05:50:48,170 - INFO - Load pretrained SentenceTransformer: sentence-transformers/use-cmlm-multilingual
Some weights of the model checkpoint at sentence-transformers/use-cmlm-multilingual were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2024-08-20 05:50:49,707 - INFO - Use pytorch device_name: cuda
2024-08-20 05:50:49,707 - INFO - Load pretrained SentenceTransformer: sentence-transformers/use-cmlm-multilingual
Some weights of the model checkpoint at sentence-transformers/use-cmlm-multilingual were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Seed set to 447
{'feature_type': <FeatureSet.SBERT_EXT1: 2>, 'agg_type': <AggregationMethod.LINEAR: 1>, 'nheads': 0, 'model_name': 'sentence-transformers/use-cmlm-multilingual', 'external_dim1': 1536, 'external_dim2': 3072, 'device': 'cuda:0', 'seed': 447, 'train_fact_path': './sample_data/trial_fact_checks.csv', 'train_post_path': './sample_data/trial_posts.csv', 'train_post2fact_mapping': './sample_data/trial_data_mapping.csv', 'train_fact_orig_emb_path': './openai-op/eval_orig-fact.pkl', 'train_fact_eng_emb_path': './openai-op/eval_eng-fact.pkl', 'train_post_l1_emb_path': './openai-op/eval_l1-post.pkl', 'train_post_l2_emb_path': './openai-op/eval_l2-post.pkl', 'eval_fact_path': './sample_data/trial_fact_checks.csv', 'eval_post_path': './sample_data/trial_posts.csv', 'eval_post2fact_mapping': './sample_data/trial_data_mapping.csv', 'eval_fact_orig_emb_path': './openai-op/eval_orig-fact.pkl', 'eval_fact_eng_emb_path': './openai-op/eval_eng-fact.pkl', 'eval_post_l1_emb_path': './openai-op/eval_l1-post.pkl', 'eval_post_l2_emb_path': './openai-op/eval_l2-post.pkl', 'loss_scale': 20, 'train_batch_size': 16, 'accumulation_steps': 1, 'eval_batch_size': 16, 'max_seq_len': 312, 'n_epochs': 2, 'wt_decay': 1e-08, 'lr': 0.0001, 'K': 5, 'log_path': './logs/447/f-SBERT_EXT1_a-LINEAR_b-16_nh-0.log', 'checkpoint_dir': './checkpoints/', 'model_checkpointing': False, 'load_checkpoint': False, 'checkpoint_path': './checkpoints/checkpoint_epoch_1.pth', 'loss_log_step': -1}
0it [00:00, ?it/s]50it [00:00, 17751.41it/s]
/home/ubuntu/miniconda3/envs/mlingual/lib/python3.11/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
2024-08-20 05:50:53,411 - INFO - Evaluation result at epoch 0:
	   post_l1_fact_eng  post_l1_fact_orig  post_l2_fact_eng  post_l2_fact_orig
	0              0.68               0.66              0.76               0.70
	1              0.74               0.76              0.80               0.76
	2              0.78               0.78              0.84               0.80
	3              0.82               0.80              0.86               0.84
	4              0.82               0.82              0.88               0.86
  0%|          | 0/9 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/9 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/9 [00:00<?, ?it/s, loss=1.19]Epoch 0:  11%|█         | 1/9 [00:00<00:03,  2.12it/s, loss=1.19]Epoch 0:  11%|█         | 1/9 [00:00<00:03,  2.12it/s, loss=1.19]Epoch 0:  11%|█         | 1/9 [00:00<00:03,  2.12it/s, loss=1.44]Epoch 0:  22%|██▏       | 2/9 [00:00<00:02,  3.38it/s, loss=1.44]Epoch 0:  22%|██▏       | 2/9 [00:00<00:02,  3.38it/s, loss=1.44]Epoch 0:  22%|██▏       | 2/9 [00:00<00:02,  3.38it/s, loss=0.865]Epoch 0:  33%|███▎      | 3/9 [00:00<00:01,  4.24it/s, loss=0.865]Epoch 0:  33%|███▎      | 3/9 [00:00<00:01,  4.24it/s, loss=0.865]Epoch 0:  33%|███▎      | 3/9 [00:00<00:01,  4.24it/s, loss=0.855]Epoch 0:  44%|████▍     | 4/9 [00:00<00:01,  4.65it/s, loss=0.855]Epoch 0:  44%|████▍     | 4/9 [00:01<00:01,  4.65it/s, loss=0.855]Epoch 0:  44%|████▍     | 4/9 [00:01<00:01,  4.65it/s, loss=0.652]Epoch 0:  56%|█████▌    | 5/9 [00:01<00:00,  5.02it/s, loss=0.652]Epoch 0:  56%|█████▌    | 5/9 [00:01<00:00,  5.02it/s, loss=0.652]Epoch 0:  56%|█████▌    | 5/9 [00:01<00:00,  5.02it/s, loss=0.493]Epoch 0:  67%|██████▋   | 6/9 [00:01<00:00,  5.19it/s, loss=0.493]Epoch 0:  67%|██████▋   | 6/9 [00:01<00:00,  5.19it/s, loss=0.493]Epoch 0:  67%|██████▋   | 6/9 [00:01<00:00,  5.19it/s, loss=0.309]Epoch 0:  78%|███████▊  | 7/9 [00:01<00:00,  5.40it/s, loss=0.309]Epoch 0:  78%|███████▊  | 7/9 [00:01<00:00,  5.40it/s, loss=0.309]Epoch 0:  78%|███████▊  | 7/9 [00:01<00:00,  5.40it/s, loss=0.239]Epoch 0:  89%|████████▉ | 8/9 [00:01<00:00,  5.44it/s, loss=0.239]Epoch 0:  89%|████████▉ | 8/9 [00:01<00:00,  5.44it/s, loss=0.239]Epoch 0:  89%|████████▉ | 8/9 [00:01<00:00,  5.44it/s, loss=5.66e-6]Epoch 0: 100%|██████████| 9/9 [00:01<00:00,  5.93it/s, loss=5.66e-6]Epoch 0: 100%|██████████| 9/9 [00:01<00:00,  4.78it/s, loss=5.66e-6]
2024-08-20 05:50:55,295 - INFO - Epoch 0: Last Batch Loss = 5.6624244280101266e-06
2024-08-20 05:50:56,675 - INFO - Evaluation result at epoch 1:
	   post_l1_fact_eng  post_l1_fact_orig  post_l2_fact_eng  post_l2_fact_orig
	0              0.94               0.94              0.94               0.94
	1              0.98               0.96              0.98               0.96
	2              1.00               1.00              1.00               1.00
	3              1.00               1.00              1.00               1.00
	4              1.00               1.00              1.00               1.00
  0%|          | 0/9 [00:00<?, ?it/s]Epoch 1:   0%|          | 0/9 [00:00<?, ?it/s]Epoch 1:   0%|          | 0/9 [00:00<?, ?it/s, loss=0.533]Epoch 1:  11%|█         | 1/9 [00:00<00:03,  2.54it/s, loss=0.533]Epoch 1:  11%|█         | 1/9 [00:00<00:03,  2.54it/s, loss=0.533]Epoch 1:  11%|█         | 1/9 [00:00<00:03,  2.54it/s, loss=0.207]Epoch 1:  22%|██▏       | 2/9 [00:00<00:01,  3.77it/s, loss=0.207]Epoch 1:  22%|██▏       | 2/9 [00:00<00:01,  3.77it/s, loss=0.207]Epoch 1:  22%|██▏       | 2/9 [00:00<00:01,  3.77it/s, loss=0.283]Epoch 1:  33%|███▎      | 3/9 [00:00<00:01,  4.44it/s, loss=0.283]Epoch 1:  33%|███▎      | 3/9 [00:00<00:01,  4.44it/s, loss=0.283]Epoch 1:  33%|███▎      | 3/9 [00:00<00:01,  4.44it/s, loss=0.126]Epoch 1:  44%|████▍     | 4/9 [00:00<00:01,  4.79it/s, loss=0.126]Epoch 1:  44%|████▍     | 4/9 [00:01<00:01,  4.79it/s, loss=0.126]Epoch 1:  44%|████▍     | 4/9 [00:01<00:01,  4.79it/s, loss=0.113]Epoch 1:  56%|█████▌    | 5/9 [00:01<00:00,  5.02it/s, loss=0.113]Epoch 1:  56%|█████▌    | 5/9 [00:01<00:00,  5.02it/s, loss=0.113]Epoch 1:  56%|█████▌    | 5/9 [00:01<00:00,  5.02it/s, loss=0.194]Epoch 1:  67%|██████▋   | 6/9 [00:01<00:00,  5.17it/s, loss=0.194]Epoch 1:  67%|██████▋   | 6/9 [00:01<00:00,  5.17it/s, loss=0.194]Epoch 1:  67%|██████▋   | 6/9 [00:01<00:00,  5.17it/s, loss=0.202]Epoch 1:  78%|███████▊  | 7/9 [00:01<00:00,  5.36it/s, loss=0.202]Epoch 1:  78%|███████▊  | 7/9 [00:01<00:00,  5.36it/s, loss=0.202]Epoch 1:  78%|███████▊  | 7/9 [00:01<00:00,  5.36it/s, loss=0.122]Epoch 1:  89%|████████▉ | 8/9 [00:01<00:00,  5.61it/s, loss=0.122]Epoch 1:  89%|████████▉ | 8/9 [00:01<00:00,  5.61it/s, loss=0.122]Epoch 1:  89%|████████▉ | 8/9 [00:01<00:00,  5.61it/s, loss=0.000411]Epoch 1: 100%|██████████| 9/9 [00:01<00:00,  6.12it/s, loss=0.000411]Epoch 1: 100%|██████████| 9/9 [00:01<00:00,  4.97it/s, loss=0.000411]
2024-08-20 05:50:58,490 - INFO - Epoch 1: Last Batch Loss = 0.00041081965900957584
2024-08-20 05:50:59,890 - INFO - Evaluation result at epoch 2:
	   post_l1_fact_eng  post_l1_fact_orig  post_l2_fact_eng  post_l2_fact_orig
	0              0.94               0.94              0.94               0.94
	1              0.98               0.98              0.98               0.98
	2              1.00               1.00              1.00               1.00
	3              1.00               1.00              1.00               1.00
	4              1.00               1.00              1.00               1.00
Running experiment with feature_type=2, agg_type=2 (ATTENTION), nheads=4, batch_size=16, lr=1e-4
2024-08-20 05:51:05,094 - INFO - Use pytorch device_name: cuda
2024-08-20 05:51:05,094 - INFO - Load pretrained SentenceTransformer: sentence-transformers/use-cmlm-multilingual
Some weights of the model checkpoint at sentence-transformers/use-cmlm-multilingual were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2024-08-20 05:51:06,777 - INFO - Use pytorch device_name: cuda
2024-08-20 05:51:06,777 - INFO - Load pretrained SentenceTransformer: sentence-transformers/use-cmlm-multilingual
Some weights of the model checkpoint at sentence-transformers/use-cmlm-multilingual were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Seed set to 447
{'feature_type': <FeatureSet.SBERT_EXT1: 2>, 'agg_type': <AggregationMethod.ATTENTION: 2>, 'nheads': 4, 'model_name': 'sentence-transformers/use-cmlm-multilingual', 'external_dim1': 1536, 'external_dim2': 3072, 'device': 'cuda:0', 'seed': 447, 'train_fact_path': './sample_data/trial_fact_checks.csv', 'train_post_path': './sample_data/trial_posts.csv', 'train_post2fact_mapping': './sample_data/trial_data_mapping.csv', 'train_fact_orig_emb_path': './openai-op/eval_orig-fact.pkl', 'train_fact_eng_emb_path': './openai-op/eval_eng-fact.pkl', 'train_post_l1_emb_path': './openai-op/eval_l1-post.pkl', 'train_post_l2_emb_path': './openai-op/eval_l2-post.pkl', 'eval_fact_path': './sample_data/trial_fact_checks.csv', 'eval_post_path': './sample_data/trial_posts.csv', 'eval_post2fact_mapping': './sample_data/trial_data_mapping.csv', 'eval_fact_orig_emb_path': './openai-op/eval_orig-fact.pkl', 'eval_fact_eng_emb_path': './openai-op/eval_eng-fact.pkl', 'eval_post_l1_emb_path': './openai-op/eval_l1-post.pkl', 'eval_post_l2_emb_path': './openai-op/eval_l2-post.pkl', 'loss_scale': 20, 'train_batch_size': 16, 'accumulation_steps': 1, 'eval_batch_size': 16, 'max_seq_len': 312, 'n_epochs': 2, 'wt_decay': 1e-08, 'lr': 0.0001, 'K': 5, 'log_path': './logs/447/f-SBERT_EXT1_a-ATTENTION_b-16_nh-4.log', 'checkpoint_dir': './checkpoints/', 'model_checkpointing': False, 'load_checkpoint': False, 'checkpoint_path': './checkpoints/checkpoint_epoch_1.pth', 'loss_log_step': -1}
0it [00:00, ?it/s]50it [00:00, 17663.20it/s]
/home/ubuntu/miniconda3/envs/mlingual/lib/python3.11/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
2024-08-20 05:51:10,389 - INFO - Evaluation result at epoch 0:
	   post_l1_fact_eng  post_l1_fact_orig  post_l2_fact_eng  post_l2_fact_orig
	0              0.56               0.52              0.64               0.60
	1              0.64               0.64              0.68               0.66
	2              0.68               0.68              0.74               0.72
	3              0.76               0.74              0.76               0.78
	4              0.82               0.80              0.78               0.78
  0%|          | 0/9 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/9 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/9 [00:00<?, ?it/s, loss=2.52]Epoch 0:  11%|█         | 1/9 [00:00<00:03,  2.13it/s, loss=2.52]Epoch 0:  11%|█         | 1/9 [00:00<00:03,  2.13it/s, loss=2.52]Epoch 0:  11%|█         | 1/9 [00:00<00:03,  2.13it/s, loss=2.55]Epoch 0:  22%|██▏       | 2/9 [00:00<00:02,  3.33it/s, loss=2.55]Epoch 0:  22%|██▏       | 2/9 [00:00<00:02,  3.33it/s, loss=2.55]Epoch 0:  22%|██▏       | 2/9 [00:00<00:02,  3.33it/s, loss=2.45]Epoch 0:  33%|███▎      | 3/9 [00:00<00:01,  4.09it/s, loss=2.45]Epoch 0:  33%|███▎      | 3/9 [00:00<00:01,  4.09it/s, loss=2.45]Epoch 0:  33%|███▎      | 3/9 [00:01<00:01,  4.09it/s, loss=2.21]Epoch 0:  44%|████▍     | 4/9 [00:01<00:01,  4.61it/s, loss=2.21]Epoch 0:  44%|████▍     | 4/9 [00:01<00:01,  4.61it/s, loss=2.21]Epoch 0:  44%|████▍     | 4/9 [00:01<00:01,  4.61it/s, loss=2.15]Epoch 0:  56%|█████▌    | 5/9 [00:01<00:00,  4.91it/s, loss=2.15]Epoch 0:  56%|█████▌    | 5/9 [00:01<00:00,  4.91it/s, loss=2.15]Epoch 0:  56%|█████▌    | 5/9 [00:01<00:00,  4.91it/s, loss=2.07]Epoch 0:  67%|██████▋   | 6/9 [00:01<00:00,  5.09it/s, loss=2.07]Epoch 0:  67%|██████▋   | 6/9 [00:01<00:00,  5.09it/s, loss=2.07]Epoch 0:  67%|██████▋   | 6/9 [00:01<00:00,  5.09it/s, loss=1.86]Epoch 0:  78%|███████▊  | 7/9 [00:01<00:00,  5.29it/s, loss=1.86]Epoch 0:  78%|███████▊  | 7/9 [00:01<00:00,  5.29it/s, loss=1.86]Epoch 0:  78%|███████▊  | 7/9 [00:01<00:00,  5.29it/s, loss=1.74]Epoch 0:  89%|████████▉ | 8/9 [00:01<00:00,  5.36it/s, loss=1.74]Epoch 0:  89%|████████▉ | 8/9 [00:01<00:00,  5.36it/s, loss=1.74]Epoch 0:  89%|████████▉ | 8/9 [00:01<00:00,  5.36it/s, loss=0.285]Epoch 0: 100%|██████████| 9/9 [00:01<00:00,  5.86it/s, loss=0.285]Epoch 0: 100%|██████████| 9/9 [00:01<00:00,  4.71it/s, loss=0.285]
2024-08-20 05:51:12,301 - INFO - Epoch 0: Last Batch Loss = 0.28494489192962646
2024-08-20 05:51:13,719 - INFO - Evaluation result at epoch 1:
	   post_l1_fact_eng  post_l1_fact_orig  post_l2_fact_eng  post_l2_fact_orig
	0              0.90               0.90              0.90               0.90
	1              0.98               0.98              0.98               0.98
	2              1.00               1.00              1.00               1.00
	3              1.00               1.00              1.00               1.00
	4              1.00               1.00              1.00               1.00
  0%|          | 0/9 [00:00<?, ?it/s]Epoch 1:   0%|          | 0/9 [00:00<?, ?it/s]Epoch 1:   0%|          | 0/9 [00:00<?, ?it/s, loss=1.11]Epoch 1:  11%|█         | 1/9 [00:00<00:03,  2.52it/s, loss=1.11]Epoch 1:  11%|█         | 1/9 [00:00<00:03,  2.52it/s, loss=1.11]Epoch 1:  11%|█         | 1/9 [00:00<00:03,  2.52it/s, loss=1.08]Epoch 1:  22%|██▏       | 2/9 [00:00<00:01,  3.81it/s, loss=1.08]Epoch 1:  22%|██▏       | 2/9 [00:00<00:01,  3.81it/s, loss=1.08]Epoch 1:  22%|██▏       | 2/9 [00:00<00:01,  3.81it/s, loss=1.09]Epoch 1:  33%|███▎      | 3/9 [00:00<00:01,  4.52it/s, loss=1.09]Epoch 1:  33%|███▎      | 3/9 [00:00<00:01,  4.52it/s, loss=1.09]Epoch 1:  33%|███▎      | 3/9 [00:00<00:01,  4.52it/s, loss=0.888]Epoch 1:  44%|████▍     | 4/9 [00:00<00:01,  4.74it/s, loss=0.888]Epoch 1:  44%|████▍     | 4/9 [00:01<00:01,  4.74it/s, loss=0.888]Epoch 1:  44%|████▍     | 4/9 [00:01<00:01,  4.74it/s, loss=0.976]Epoch 1:  56%|█████▌    | 5/9 [00:01<00:00,  5.08it/s, loss=0.976]Epoch 1:  56%|█████▌    | 5/9 [00:01<00:00,  5.08it/s, loss=0.976]Epoch 1:  56%|█████▌    | 5/9 [00:01<00:00,  5.08it/s, loss=0.893]Epoch 1:  67%|██████▋   | 6/9 [00:01<00:00,  5.26it/s, loss=0.893]Epoch 1:  67%|██████▋   | 6/9 [00:01<00:00,  5.26it/s, loss=0.893]Epoch 1:  67%|██████▋   | 6/9 [00:01<00:00,  5.26it/s, loss=0.947]Epoch 1:  78%|███████▊  | 7/9 [00:01<00:00,  5.40it/s, loss=0.947]Epoch 1:  78%|███████▊  | 7/9 [00:01<00:00,  5.40it/s, loss=0.947]Epoch 1:  78%|███████▊  | 7/9 [00:01<00:00,  5.40it/s, loss=0.908]Epoch 1:  89%|████████▉ | 8/9 [00:01<00:00,  5.40it/s, loss=0.908]Epoch 1:  89%|████████▉ | 8/9 [00:01<00:00,  5.40it/s, loss=0.908]Epoch 1:  89%|████████▉ | 8/9 [00:01<00:00,  5.40it/s, loss=0.107]Epoch 1: 100%|██████████| 9/9 [00:01<00:00,  5.90it/s, loss=0.107]Epoch 1: 100%|██████████| 9/9 [00:01<00:00,  4.91it/s, loss=0.107]
2024-08-20 05:51:15,554 - INFO - Epoch 1: Last Batch Loss = 0.10651499032974243
2024-08-20 05:51:16,941 - INFO - Evaluation result at epoch 2:
	   post_l1_fact_eng  post_l1_fact_orig  post_l2_fact_eng  post_l2_fact_orig
	0              0.90               0.90              0.90               0.90
	1              0.98               0.98              0.98               0.98
	2              1.00               1.00              1.00               1.00
	3              1.00               1.00              1.00               1.00
	4              1.00               1.00              1.00               1.00
Running experiment with feature_type=2, agg_type=2 (ATTENTION), nheads=8, batch_size=16, lr=1e-4
2024-08-20 05:51:22,154 - INFO - Use pytorch device_name: cuda
2024-08-20 05:51:22,154 - INFO - Load pretrained SentenceTransformer: sentence-transformers/use-cmlm-multilingual
Some weights of the model checkpoint at sentence-transformers/use-cmlm-multilingual were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2024-08-20 05:51:23,734 - INFO - Use pytorch device_name: cuda
2024-08-20 05:51:23,734 - INFO - Load pretrained SentenceTransformer: sentence-transformers/use-cmlm-multilingual
Some weights of the model checkpoint at sentence-transformers/use-cmlm-multilingual were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Seed set to 447
{'feature_type': <FeatureSet.SBERT_EXT1: 2>, 'agg_type': <AggregationMethod.ATTENTION: 2>, 'nheads': 8, 'model_name': 'sentence-transformers/use-cmlm-multilingual', 'external_dim1': 1536, 'external_dim2': 3072, 'device': 'cuda:0', 'seed': 447, 'train_fact_path': './sample_data/trial_fact_checks.csv', 'train_post_path': './sample_data/trial_posts.csv', 'train_post2fact_mapping': './sample_data/trial_data_mapping.csv', 'train_fact_orig_emb_path': './openai-op/eval_orig-fact.pkl', 'train_fact_eng_emb_path': './openai-op/eval_eng-fact.pkl', 'train_post_l1_emb_path': './openai-op/eval_l1-post.pkl', 'train_post_l2_emb_path': './openai-op/eval_l2-post.pkl', 'eval_fact_path': './sample_data/trial_fact_checks.csv', 'eval_post_path': './sample_data/trial_posts.csv', 'eval_post2fact_mapping': './sample_data/trial_data_mapping.csv', 'eval_fact_orig_emb_path': './openai-op/eval_orig-fact.pkl', 'eval_fact_eng_emb_path': './openai-op/eval_eng-fact.pkl', 'eval_post_l1_emb_path': './openai-op/eval_l1-post.pkl', 'eval_post_l2_emb_path': './openai-op/eval_l2-post.pkl', 'loss_scale': 20, 'train_batch_size': 16, 'accumulation_steps': 1, 'eval_batch_size': 16, 'max_seq_len': 312, 'n_epochs': 2, 'wt_decay': 1e-08, 'lr': 0.0001, 'K': 5, 'log_path': './logs/447/f-SBERT_EXT1_a-ATTENTION_b-16_nh-8.log', 'checkpoint_dir': './checkpoints/', 'model_checkpointing': False, 'load_checkpoint': False, 'checkpoint_path': './checkpoints/checkpoint_epoch_1.pth', 'loss_log_step': -1}
0it [00:00, ?it/s]50it [00:00, 17754.42it/s]
/home/ubuntu/miniconda3/envs/mlingual/lib/python3.11/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
2024-08-20 05:51:27,352 - INFO - Evaluation result at epoch 0:
	   post_l1_fact_eng  post_l1_fact_orig  post_l2_fact_eng  post_l2_fact_orig
	0              0.56               0.52              0.64               0.60
	1              0.64               0.64              0.68               0.66
	2              0.68               0.68              0.74               0.72
	3              0.76               0.74              0.76               0.78
	4              0.82               0.80              0.78               0.78
  0%|          | 0/9 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/9 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/9 [00:00<?, ?it/s, loss=2.52]Epoch 0:  11%|█         | 1/9 [00:00<00:03,  2.12it/s, loss=2.52]Epoch 0:  11%|█         | 1/9 [00:00<00:03,  2.12it/s, loss=2.52]Epoch 0:  11%|█         | 1/9 [00:00<00:03,  2.12it/s, loss=2.55]Epoch 0:  22%|██▏       | 2/9 [00:00<00:02,  3.33it/s, loss=2.55]Epoch 0:  22%|██▏       | 2/9 [00:00<00:02,  3.33it/s, loss=2.55]Epoch 0:  22%|██▏       | 2/9 [00:00<00:02,  3.33it/s, loss=2.45]Epoch 0:  33%|███▎      | 3/9 [00:00<00:01,  4.10it/s, loss=2.45]Epoch 0:  33%|███▎      | 3/9 [00:01<00:01,  4.10it/s, loss=2.45]Epoch 0:  33%|███▎      | 3/9 [00:01<00:01,  4.10it/s, loss=2.21]Epoch 0:  44%|████▍     | 4/9 [00:01<00:01,  4.59it/s, loss=2.21]Epoch 0:  44%|████▍     | 4/9 [00:01<00:01,  4.59it/s, loss=2.21]Epoch 0:  44%|████▍     | 4/9 [00:01<00:01,  4.59it/s, loss=2.15]Epoch 0:  56%|█████▌    | 5/9 [00:01<00:00,  4.89it/s, loss=2.15]Epoch 0:  56%|█████▌    | 5/9 [00:01<00:00,  4.89it/s, loss=2.15]Epoch 0:  56%|█████▌    | 5/9 [00:01<00:00,  4.89it/s, loss=2.07]Epoch 0:  67%|██████▋   | 6/9 [00:01<00:00,  5.08it/s, loss=2.07]Epoch 0:  67%|██████▋   | 6/9 [00:01<00:00,  5.08it/s, loss=2.07]Epoch 0:  67%|██████▋   | 6/9 [00:01<00:00,  5.08it/s, loss=1.86]Epoch 0:  78%|███████▊  | 7/9 [00:01<00:00,  5.28it/s, loss=1.86]Epoch 0:  78%|███████▊  | 7/9 [00:01<00:00,  5.28it/s, loss=1.86]Epoch 0:  78%|███████▊  | 7/9 [00:01<00:00,  5.28it/s, loss=1.74]Epoch 0:  89%|████████▉ | 8/9 [00:01<00:00,  5.34it/s, loss=1.74]Epoch 0:  89%|████████▉ | 8/9 [00:01<00:00,  5.34it/s, loss=1.74]Epoch 0:  89%|████████▉ | 8/9 [00:01<00:00,  5.34it/s, loss=0.285]Epoch 0: 100%|██████████| 9/9 [00:01<00:00,  5.86it/s, loss=0.285]Epoch 0: 100%|██████████| 9/9 [00:01<00:00,  4.71it/s, loss=0.285]
2024-08-20 05:51:29,266 - INFO - Epoch 0: Last Batch Loss = 0.28495287895202637
2024-08-20 05:51:30,674 - INFO - Evaluation result at epoch 1:
	   post_l1_fact_eng  post_l1_fact_orig  post_l2_fact_eng  post_l2_fact_orig
	0              0.90               0.90              0.90               0.90
	1              0.98               0.98              0.98               0.98
	2              1.00               1.00              1.00               1.00
	3              1.00               1.00              1.00               1.00
	4              1.00               1.00              1.00               1.00
  0%|          | 0/9 [00:00<?, ?it/s]Epoch 1:   0%|          | 0/9 [00:00<?, ?it/s]Epoch 1:   0%|          | 0/9 [00:00<?, ?it/s, loss=1.11]Epoch 1:  11%|█         | 1/9 [00:00<00:03,  2.40it/s, loss=1.11]Epoch 1:  11%|█         | 1/9 [00:00<00:03,  2.40it/s, loss=1.11]Epoch 1:  11%|█         | 1/9 [00:00<00:03,  2.40it/s, loss=1.08]Epoch 1:  22%|██▏       | 2/9 [00:00<00:01,  3.73it/s, loss=1.08]Epoch 1:  22%|██▏       | 2/9 [00:00<00:01,  3.73it/s, loss=1.08]Epoch 1:  22%|██▏       | 2/9 [00:00<00:01,  3.73it/s, loss=1.09]Epoch 1:  33%|███▎      | 3/9 [00:00<00:01,  4.45it/s, loss=1.09]Epoch 1:  33%|███▎      | 3/9 [00:00<00:01,  4.45it/s, loss=1.09]Epoch 1:  33%|███▎      | 3/9 [00:00<00:01,  4.45it/s, loss=0.888]Epoch 1:  44%|████▍     | 4/9 [00:00<00:01,  4.69it/s, loss=0.888]Epoch 1:  44%|████▍     | 4/9 [00:01<00:01,  4.69it/s, loss=0.888]Epoch 1:  44%|████▍     | 4/9 [00:01<00:01,  4.69it/s, loss=0.976]Epoch 1:  56%|█████▌    | 5/9 [00:01<00:00,  5.05it/s, loss=0.976]Epoch 1:  56%|█████▌    | 5/9 [00:01<00:00,  5.05it/s, loss=0.976]Epoch 1:  56%|█████▌    | 5/9 [00:01<00:00,  5.05it/s, loss=0.893]Epoch 1:  67%|██████▋   | 6/9 [00:01<00:00,  5.24it/s, loss=0.893]Epoch 1:  67%|██████▋   | 6/9 [00:01<00:00,  5.24it/s, loss=0.893]Epoch 1:  67%|██████▋   | 6/9 [00:01<00:00,  5.24it/s, loss=0.947]Epoch 1:  78%|███████▊  | 7/9 [00:01<00:00,  5.38it/s, loss=0.947]Epoch 1:  78%|███████▊  | 7/9 [00:01<00:00,  5.38it/s, loss=0.947]Epoch 1:  78%|███████▊  | 7/9 [00:01<00:00,  5.38it/s, loss=0.908]Epoch 1:  89%|████████▉ | 8/9 [00:01<00:00,  5.39it/s, loss=0.908]Epoch 1:  89%|████████▉ | 8/9 [00:01<00:00,  5.39it/s, loss=0.908]Epoch 1:  89%|████████▉ | 8/9 [00:01<00:00,  5.39it/s, loss=0.107]Epoch 1: 100%|██████████| 9/9 [00:01<00:00,  5.90it/s, loss=0.107]Epoch 1: 100%|██████████| 9/9 [00:01<00:00,  4.88it/s, loss=0.107]
2024-08-20 05:51:32,520 - INFO - Epoch 1: Last Batch Loss = 0.10653470456600189
2024-08-20 05:51:33,902 - INFO - Evaluation result at epoch 2:
	   post_l1_fact_eng  post_l1_fact_orig  post_l2_fact_eng  post_l2_fact_orig
	0              0.90               0.90              0.90               0.90
	1              0.98               0.98              0.98               0.98
	2              1.00               1.00              1.00               1.00
	3              1.00               1.00              1.00               1.00
	4              1.00               1.00              1.00               1.00
Running experiment with feature_type=3, agg_type=1 (LINEAR), batch_size=16, lr=1e-4
2024-08-20 05:51:39,090 - INFO - Use pytorch device_name: cuda
2024-08-20 05:51:39,091 - INFO - Load pretrained SentenceTransformer: sentence-transformers/use-cmlm-multilingual
Some weights of the model checkpoint at sentence-transformers/use-cmlm-multilingual were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2024-08-20 05:51:40,643 - INFO - Use pytorch device_name: cuda
2024-08-20 05:51:40,643 - INFO - Load pretrained SentenceTransformer: sentence-transformers/use-cmlm-multilingual
Some weights of the model checkpoint at sentence-transformers/use-cmlm-multilingual were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Seed set to 447
{'feature_type': <FeatureSet.SBERT_EXT2: 3>, 'agg_type': <AggregationMethod.LINEAR: 1>, 'nheads': 0, 'model_name': 'sentence-transformers/use-cmlm-multilingual', 'external_dim1': 1536, 'external_dim2': 3072, 'device': 'cuda:0', 'seed': 447, 'train_fact_path': './sample_data/trial_fact_checks.csv', 'train_post_path': './sample_data/trial_posts.csv', 'train_post2fact_mapping': './sample_data/trial_data_mapping.csv', 'train_fact_orig_emb_path': './openai-op/eval_orig-fact.pkl', 'train_fact_eng_emb_path': './openai-op/eval_eng-fact.pkl', 'train_post_l1_emb_path': './openai-op/eval_l1-post.pkl', 'train_post_l2_emb_path': './openai-op/eval_l2-post.pkl', 'eval_fact_path': './sample_data/trial_fact_checks.csv', 'eval_post_path': './sample_data/trial_posts.csv', 'eval_post2fact_mapping': './sample_data/trial_data_mapping.csv', 'eval_fact_orig_emb_path': './openai-op/eval_orig-fact.pkl', 'eval_fact_eng_emb_path': './openai-op/eval_eng-fact.pkl', 'eval_post_l1_emb_path': './openai-op/eval_l1-post.pkl', 'eval_post_l2_emb_path': './openai-op/eval_l2-post.pkl', 'loss_scale': 20, 'train_batch_size': 16, 'accumulation_steps': 1, 'eval_batch_size': 16, 'max_seq_len': 312, 'n_epochs': 2, 'wt_decay': 1e-08, 'lr': 0.0001, 'K': 5, 'log_path': './logs/447/f-SBERT_EXT2_a-LINEAR_b-16_nh-0.log', 'checkpoint_dir': './checkpoints/', 'model_checkpointing': False, 'load_checkpoint': False, 'checkpoint_path': './checkpoints/checkpoint_epoch_1.pth', 'loss_log_step': -1}
0it [00:00, ?it/s]50it [00:00, 17509.83it/s]
/home/ubuntu/miniconda3/envs/mlingual/lib/python3.11/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
2024-08-20 05:51:44,237 - INFO - Evaluation result at epoch 0:
	   post_l1_fact_eng  post_l1_fact_orig  post_l2_fact_eng  post_l2_fact_orig
	0              0.62               0.64              0.74               0.70
	1              0.72               0.72              0.76               0.74
	2              0.78               0.74              0.82               0.78
	3              0.80               0.78              0.82               0.82
	4              0.80               0.80              0.86               0.82
  0%|          | 0/9 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/9 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/9 [00:00<?, ?it/s, loss=1.29]Epoch 0:  11%|█         | 1/9 [00:00<00:03,  2.05it/s, loss=1.29]Epoch 0:  11%|█         | 1/9 [00:00<00:03,  2.05it/s, loss=1.29]Epoch 0:  11%|█         | 1/9 [00:00<00:03,  2.05it/s, loss=1.44]Epoch 0:  22%|██▏       | 2/9 [00:00<00:02,  3.37it/s, loss=1.44]Epoch 0:  22%|██▏       | 2/9 [00:00<00:02,  3.37it/s, loss=1.44]Epoch 0:  22%|██▏       | 2/9 [00:00<00:02,  3.37it/s, loss=0.88]Epoch 0:  33%|███▎      | 3/9 [00:00<00:01,  4.22it/s, loss=0.88]Epoch 0:  33%|███▎      | 3/9 [00:00<00:01,  4.22it/s, loss=0.88]Epoch 0:  33%|███▎      | 3/9 [00:01<00:01,  4.22it/s, loss=0.91]Epoch 0:  44%|████▍     | 4/9 [00:01<00:01,  4.62it/s, loss=0.91]Epoch 0:  44%|████▍     | 4/9 [00:01<00:01,  4.62it/s, loss=0.91]Epoch 0:  44%|████▍     | 4/9 [00:01<00:01,  4.62it/s, loss=0.688]Epoch 0:  56%|█████▌    | 5/9 [00:01<00:00,  5.00it/s, loss=0.688]Epoch 0:  56%|█████▌    | 5/9 [00:01<00:00,  5.00it/s, loss=0.688]Epoch 0:  56%|█████▌    | 5/9 [00:01<00:00,  5.00it/s, loss=0.483]Epoch 0:  67%|██████▋   | 6/9 [00:01<00:00,  5.17it/s, loss=0.483]Epoch 0:  67%|██████▋   | 6/9 [00:01<00:00,  5.17it/s, loss=0.483]Epoch 0:  67%|██████▋   | 6/9 [00:01<00:00,  5.17it/s, loss=0.339]Epoch 0:  78%|███████▊  | 7/9 [00:01<00:00,  5.37it/s, loss=0.339]Epoch 0:  78%|███████▊  | 7/9 [00:01<00:00,  5.37it/s, loss=0.339]Epoch 0:  78%|███████▊  | 7/9 [00:01<00:00,  5.37it/s, loss=0.219]Epoch 0:  89%|████████▉ | 8/9 [00:01<00:00,  5.43it/s, loss=0.219]Epoch 0:  89%|████████▉ | 8/9 [00:01<00:00,  5.43it/s, loss=0.219]Epoch 0:  89%|████████▉ | 8/9 [00:01<00:00,  5.43it/s, loss=1.11e-5]Epoch 0: 100%|██████████| 9/9 [00:01<00:00,  5.92it/s, loss=1.11e-5]Epoch 0: 100%|██████████| 9/9 [00:01<00:00,  4.76it/s, loss=1.11e-5]
2024-08-20 05:51:46,128 - INFO - Epoch 0: Last Batch Loss = 1.1086391168646514e-05
2024-08-20 05:51:47,526 - INFO - Evaluation result at epoch 1:
	   post_l1_fact_eng  post_l1_fact_orig  post_l2_fact_eng  post_l2_fact_orig
	0              0.94               0.94              0.94               0.94
	1              0.98               0.98              0.98               0.98
	2              1.00               1.00              1.00               1.00
	3              1.00               1.00              1.00               1.00
	4              1.00               1.00              1.00               1.00
  0%|          | 0/9 [00:00<?, ?it/s]Epoch 1:   0%|          | 0/9 [00:00<?, ?it/s]Epoch 1:   0%|          | 0/9 [00:00<?, ?it/s, loss=0.491]Epoch 1:  11%|█         | 1/9 [00:00<00:03,  2.50it/s, loss=0.491]Epoch 1:  11%|█         | 1/9 [00:00<00:03,  2.50it/s, loss=0.491]Epoch 1:  11%|█         | 1/9 [00:00<00:03,  2.50it/s, loss=0.209]Epoch 1:  22%|██▏       | 2/9 [00:00<00:01,  3.73it/s, loss=0.209]Epoch 1:  22%|██▏       | 2/9 [00:00<00:01,  3.73it/s, loss=0.209]Epoch 1:  22%|██▏       | 2/9 [00:00<00:01,  3.73it/s, loss=0.277]Epoch 1:  33%|███▎      | 3/9 [00:00<00:01,  4.41it/s, loss=0.277]Epoch 1:  33%|███▎      | 3/9 [00:00<00:01,  4.41it/s, loss=0.277]Epoch 1:  33%|███▎      | 3/9 [00:00<00:01,  4.41it/s, loss=0.117]Epoch 1:  44%|████▍     | 4/9 [00:00<00:01,  4.72it/s, loss=0.117]Epoch 1:  44%|████▍     | 4/9 [00:01<00:01,  4.72it/s, loss=0.117]Epoch 1:  44%|████▍     | 4/9 [00:01<00:01,  4.72it/s, loss=0.109]Epoch 1:  56%|█████▌    | 5/9 [00:01<00:00,  4.97it/s, loss=0.109]Epoch 1:  56%|█████▌    | 5/9 [00:01<00:00,  4.97it/s, loss=0.109]Epoch 1:  56%|█████▌    | 5/9 [00:01<00:00,  4.97it/s, loss=0.183]Epoch 1:  67%|██████▋   | 6/9 [00:01<00:00,  5.21it/s, loss=0.183]Epoch 1:  67%|██████▋   | 6/9 [00:01<00:00,  5.21it/s, loss=0.183]Epoch 1:  67%|██████▋   | 6/9 [00:01<00:00,  5.21it/s, loss=0.198]Epoch 1:  78%|███████▊  | 7/9 [00:01<00:00,  5.39it/s, loss=0.198]Epoch 1:  78%|███████▊  | 7/9 [00:01<00:00,  5.39it/s, loss=0.198]Epoch 1:  78%|███████▊  | 7/9 [00:01<00:00,  5.39it/s, loss=0.116]Epoch 1:  89%|████████▉ | 8/9 [00:01<00:00,  5.60it/s, loss=0.116]Epoch 1:  89%|████████▉ | 8/9 [00:01<00:00,  5.60it/s, loss=0.116]Epoch 1:  89%|████████▉ | 8/9 [00:01<00:00,  5.60it/s, loss=0.000519]Epoch 1: 100%|██████████| 9/9 [00:01<00:00,  6.10it/s, loss=0.000519]Epoch 1: 100%|██████████| 9/9 [00:01<00:00,  4.94it/s, loss=0.000519]
2024-08-20 05:51:49,350 - INFO - Epoch 1: Last Batch Loss = 0.0005186384078115225
2024-08-20 05:51:50,739 - INFO - Evaluation result at epoch 2:
	   post_l1_fact_eng  post_l1_fact_orig  post_l2_fact_eng  post_l2_fact_orig
	0              0.94               0.94              0.94               0.94
	1              0.98               0.98              0.98               0.98
	2              1.00               1.00              1.00               1.00
	3              1.00               1.00              1.00               1.00
	4              1.00               1.00              1.00               1.00
Running experiment with feature_type=3, agg_type=2 (ATTENTION), nheads=4, batch_size=16, lr=1e-4
2024-08-20 05:51:55,936 - INFO - Use pytorch device_name: cuda
2024-08-20 05:51:55,936 - INFO - Load pretrained SentenceTransformer: sentence-transformers/use-cmlm-multilingual
Some weights of the model checkpoint at sentence-transformers/use-cmlm-multilingual were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2024-08-20 05:51:57,463 - INFO - Use pytorch device_name: cuda
2024-08-20 05:51:57,463 - INFO - Load pretrained SentenceTransformer: sentence-transformers/use-cmlm-multilingual
Some weights of the model checkpoint at sentence-transformers/use-cmlm-multilingual were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Seed set to 447
{'feature_type': <FeatureSet.SBERT_EXT2: 3>, 'agg_type': <AggregationMethod.ATTENTION: 2>, 'nheads': 4, 'model_name': 'sentence-transformers/use-cmlm-multilingual', 'external_dim1': 1536, 'external_dim2': 3072, 'device': 'cuda:0', 'seed': 447, 'train_fact_path': './sample_data/trial_fact_checks.csv', 'train_post_path': './sample_data/trial_posts.csv', 'train_post2fact_mapping': './sample_data/trial_data_mapping.csv', 'train_fact_orig_emb_path': './openai-op/eval_orig-fact.pkl', 'train_fact_eng_emb_path': './openai-op/eval_eng-fact.pkl', 'train_post_l1_emb_path': './openai-op/eval_l1-post.pkl', 'train_post_l2_emb_path': './openai-op/eval_l2-post.pkl', 'eval_fact_path': './sample_data/trial_fact_checks.csv', 'eval_post_path': './sample_data/trial_posts.csv', 'eval_post2fact_mapping': './sample_data/trial_data_mapping.csv', 'eval_fact_orig_emb_path': './openai-op/eval_orig-fact.pkl', 'eval_fact_eng_emb_path': './openai-op/eval_eng-fact.pkl', 'eval_post_l1_emb_path': './openai-op/eval_l1-post.pkl', 'eval_post_l2_emb_path': './openai-op/eval_l2-post.pkl', 'loss_scale': 20, 'train_batch_size': 16, 'accumulation_steps': 1, 'eval_batch_size': 16, 'max_seq_len': 312, 'n_epochs': 2, 'wt_decay': 1e-08, 'lr': 0.0001, 'K': 5, 'log_path': './logs/447/f-SBERT_EXT2_a-ATTENTION_b-16_nh-4.log', 'checkpoint_dir': './checkpoints/', 'model_checkpointing': False, 'load_checkpoint': False, 'checkpoint_path': './checkpoints/checkpoint_epoch_1.pth', 'loss_log_step': -1}
0it [00:00, ?it/s]50it [00:00, 17823.83it/s]
/home/ubuntu/miniconda3/envs/mlingual/lib/python3.11/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
2024-08-20 05:52:01,083 - INFO - Evaluation result at epoch 0:
	   post_l1_fact_eng  post_l1_fact_orig  post_l2_fact_eng  post_l2_fact_orig
	0              0.52               0.54              0.60               0.58
	1              0.62               0.62              0.68               0.66
	2              0.70               0.68              0.74               0.70
	3              0.78               0.72              0.78               0.76
	4              0.82               0.78              0.80               0.78
  0%|          | 0/9 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/9 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/9 [00:00<?, ?it/s, loss=2.54]Epoch 0:  11%|█         | 1/9 [00:00<00:03,  2.18it/s, loss=2.54]Epoch 0:  11%|█         | 1/9 [00:00<00:03,  2.18it/s, loss=2.54]Epoch 0:  11%|█         | 1/9 [00:00<00:03,  2.18it/s, loss=2.57]Epoch 0:  22%|██▏       | 2/9 [00:00<00:02,  3.38it/s, loss=2.57]Epoch 0:  22%|██▏       | 2/9 [00:00<00:02,  3.38it/s, loss=2.57]Epoch 0:  22%|██▏       | 2/9 [00:00<00:02,  3.38it/s, loss=2.48]Epoch 0:  33%|███▎      | 3/9 [00:00<00:01,  4.13it/s, loss=2.48]Epoch 0:  33%|███▎      | 3/9 [00:00<00:01,  4.13it/s, loss=2.48]Epoch 0:  33%|███▎      | 3/9 [00:00<00:01,  4.13it/s, loss=2.26]Epoch 0:  44%|████▍     | 4/9 [00:00<00:01,  4.63it/s, loss=2.26]Epoch 0:  44%|████▍     | 4/9 [00:01<00:01,  4.63it/s, loss=2.26]Epoch 0:  44%|████▍     | 4/9 [00:01<00:01,  4.63it/s, loss=2.16]Epoch 0:  56%|█████▌    | 5/9 [00:01<00:00,  4.92it/s, loss=2.16]Epoch 0:  56%|█████▌    | 5/9 [00:01<00:00,  4.92it/s, loss=2.16]Epoch 0:  56%|█████▌    | 5/9 [00:01<00:00,  4.92it/s, loss=2.07]Epoch 0:  67%|██████▋   | 6/9 [00:01<00:00,  5.09it/s, loss=2.07]Epoch 0:  67%|██████▋   | 6/9 [00:01<00:00,  5.09it/s, loss=2.07]Epoch 0:  67%|██████▋   | 6/9 [00:01<00:00,  5.09it/s, loss=1.85]Epoch 0:  78%|███████▊  | 7/9 [00:01<00:00,  5.28it/s, loss=1.85]Epoch 0:  78%|███████▊  | 7/9 [00:01<00:00,  5.28it/s, loss=1.85]Epoch 0:  78%|███████▊  | 7/9 [00:01<00:00,  5.28it/s, loss=1.76]Epoch 0:  89%|████████▉ | 8/9 [00:01<00:00,  5.34it/s, loss=1.76]Epoch 0:  89%|████████▉ | 8/9 [00:01<00:00,  5.34it/s, loss=1.76]Epoch 0:  89%|████████▉ | 8/9 [00:01<00:00,  5.34it/s, loss=0.348]Epoch 0: 100%|██████████| 9/9 [00:01<00:00,  5.84it/s, loss=0.348]Epoch 0: 100%|██████████| 9/9 [00:01<00:00,  4.73it/s, loss=0.348]
2024-08-20 05:52:02,990 - INFO - Epoch 0: Last Batch Loss = 0.34785354137420654
2024-08-20 05:52:04,375 - INFO - Evaluation result at epoch 1:
	   post_l1_fact_eng  post_l1_fact_orig  post_l2_fact_eng  post_l2_fact_orig
	0              0.88               0.90              0.90               0.90
	1              0.92               0.92              0.94               0.94
	2              0.98               0.98              1.00               1.00
	3              1.00               1.00              1.00               1.00
	4              1.00               1.00              1.00               1.00
  0%|          | 0/9 [00:00<?, ?it/s]Epoch 1:   0%|          | 0/9 [00:00<?, ?it/s]Epoch 1:   0%|          | 0/9 [00:00<?, ?it/s, loss=1.11]Epoch 1:  11%|█         | 1/9 [00:00<00:03,  2.55it/s, loss=1.11]Epoch 1:  11%|█         | 1/9 [00:00<00:03,  2.55it/s, loss=1.11]Epoch 1:  11%|█         | 1/9 [00:00<00:03,  2.55it/s, loss=1.12]Epoch 1:  22%|██▏       | 2/9 [00:00<00:01,  3.84it/s, loss=1.12]Epoch 1:  22%|██▏       | 2/9 [00:00<00:01,  3.84it/s, loss=1.12]Epoch 1:  22%|██▏       | 2/9 [00:00<00:01,  3.84it/s, loss=1.13]Epoch 1:  33%|███▎      | 3/9 [00:00<00:01,  4.53it/s, loss=1.13]Epoch 1:  33%|███▎      | 3/9 [00:00<00:01,  4.53it/s, loss=1.13]Epoch 1:  33%|███▎      | 3/9 [00:00<00:01,  4.53it/s, loss=0.897]Epoch 1:  44%|████▍     | 4/9 [00:00<00:01,  4.73it/s, loss=0.897]Epoch 1:  44%|████▍     | 4/9 [00:01<00:01,  4.73it/s, loss=0.897]Epoch 1:  44%|████▍     | 4/9 [00:01<00:01,  4.73it/s, loss=0.963]Epoch 1:  56%|█████▌    | 5/9 [00:01<00:00,  5.07it/s, loss=0.963]Epoch 1:  56%|█████▌    | 5/9 [00:01<00:00,  5.07it/s, loss=0.963]Epoch 1:  56%|█████▌    | 5/9 [00:01<00:00,  5.07it/s, loss=0.905]Epoch 1:  67%|██████▋   | 6/9 [00:01<00:00,  5.25it/s, loss=0.905]Epoch 1:  67%|██████▋   | 6/9 [00:01<00:00,  5.25it/s, loss=0.905]Epoch 1:  67%|██████▋   | 6/9 [00:01<00:00,  5.25it/s, loss=0.963]Epoch 1:  78%|███████▊  | 7/9 [00:01<00:00,  5.39it/s, loss=0.963]Epoch 1:  78%|███████▊  | 7/9 [00:01<00:00,  5.39it/s, loss=0.963]Epoch 1:  78%|███████▊  | 7/9 [00:01<00:00,  5.39it/s, loss=0.945]Epoch 1:  89%|████████▉ | 8/9 [00:01<00:00,  5.40it/s, loss=0.945]Epoch 1:  89%|████████▉ | 8/9 [00:01<00:00,  5.40it/s, loss=0.945]Epoch 1:  89%|████████▉ | 8/9 [00:01<00:00,  5.40it/s, loss=0.0609]Epoch 1: 100%|██████████| 9/9 [00:01<00:00,  5.90it/s, loss=0.0609]Epoch 1: 100%|██████████| 9/9 [00:01<00:00,  4.93it/s, loss=0.0609]
2024-08-20 05:52:06,204 - INFO - Epoch 1: Last Batch Loss = 0.06086808815598488
2024-08-20 05:52:07,581 - INFO - Evaluation result at epoch 2:
	   post_l1_fact_eng  post_l1_fact_orig  post_l2_fact_eng  post_l2_fact_orig
	0              0.90               0.90              0.90               0.90
	1              0.94               0.94              0.94               0.94
	2              0.98               0.98              0.98               0.98
	3              1.00               1.00              1.00               1.00
	4              1.00               1.00              1.00               1.00
Running experiment with feature_type=3, agg_type=2 (ATTENTION), nheads=8, batch_size=16, lr=1e-4
2024-08-20 05:52:12,804 - INFO - Use pytorch device_name: cuda
2024-08-20 05:52:12,805 - INFO - Load pretrained SentenceTransformer: sentence-transformers/use-cmlm-multilingual
Some weights of the model checkpoint at sentence-transformers/use-cmlm-multilingual were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2024-08-20 05:52:14,344 - INFO - Use pytorch device_name: cuda
2024-08-20 05:52:14,345 - INFO - Load pretrained SentenceTransformer: sentence-transformers/use-cmlm-multilingual
Some weights of the model checkpoint at sentence-transformers/use-cmlm-multilingual were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Seed set to 447
{'feature_type': <FeatureSet.SBERT_EXT2: 3>, 'agg_type': <AggregationMethod.ATTENTION: 2>, 'nheads': 8, 'model_name': 'sentence-transformers/use-cmlm-multilingual', 'external_dim1': 1536, 'external_dim2': 3072, 'device': 'cuda:0', 'seed': 447, 'train_fact_path': './sample_data/trial_fact_checks.csv', 'train_post_path': './sample_data/trial_posts.csv', 'train_post2fact_mapping': './sample_data/trial_data_mapping.csv', 'train_fact_orig_emb_path': './openai-op/eval_orig-fact.pkl', 'train_fact_eng_emb_path': './openai-op/eval_eng-fact.pkl', 'train_post_l1_emb_path': './openai-op/eval_l1-post.pkl', 'train_post_l2_emb_path': './openai-op/eval_l2-post.pkl', 'eval_fact_path': './sample_data/trial_fact_checks.csv', 'eval_post_path': './sample_data/trial_posts.csv', 'eval_post2fact_mapping': './sample_data/trial_data_mapping.csv', 'eval_fact_orig_emb_path': './openai-op/eval_orig-fact.pkl', 'eval_fact_eng_emb_path': './openai-op/eval_eng-fact.pkl', 'eval_post_l1_emb_path': './openai-op/eval_l1-post.pkl', 'eval_post_l2_emb_path': './openai-op/eval_l2-post.pkl', 'loss_scale': 20, 'train_batch_size': 16, 'accumulation_steps': 1, 'eval_batch_size': 16, 'max_seq_len': 312, 'n_epochs': 2, 'wt_decay': 1e-08, 'lr': 0.0001, 'K': 5, 'log_path': './logs/447/f-SBERT_EXT2_a-ATTENTION_b-16_nh-8.log', 'checkpoint_dir': './checkpoints/', 'model_checkpointing': False, 'load_checkpoint': False, 'checkpoint_path': './checkpoints/checkpoint_epoch_1.pth', 'loss_log_step': -1}
0it [00:00, ?it/s]50it [00:00, 17464.62it/s]
/home/ubuntu/miniconda3/envs/mlingual/lib/python3.11/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
2024-08-20 05:52:18,019 - INFO - Evaluation result at epoch 0:
	   post_l1_fact_eng  post_l1_fact_orig  post_l2_fact_eng  post_l2_fact_orig
	0              0.52               0.54              0.60               0.58
	1              0.62               0.62              0.68               0.66
	2              0.70               0.68              0.74               0.70
	3              0.78               0.72              0.78               0.76
	4              0.82               0.78              0.80               0.78
  0%|          | 0/9 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/9 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/9 [00:00<?, ?it/s, loss=2.54]Epoch 0:  11%|█         | 1/9 [00:00<00:03,  2.09it/s, loss=2.54]Epoch 0:  11%|█         | 1/9 [00:00<00:03,  2.09it/s, loss=2.54]Epoch 0:  11%|█         | 1/9 [00:00<00:03,  2.09it/s, loss=2.57]Epoch 0:  22%|██▏       | 2/9 [00:00<00:02,  3.30it/s, loss=2.57]Epoch 0:  22%|██▏       | 2/9 [00:00<00:02,  3.30it/s, loss=2.57]Epoch 0:  22%|██▏       | 2/9 [00:00<00:02,  3.30it/s, loss=2.48]Epoch 0:  33%|███▎      | 3/9 [00:00<00:01,  4.09it/s, loss=2.48]Epoch 0:  33%|███▎      | 3/9 [00:00<00:01,  4.09it/s, loss=2.48]Epoch 0:  33%|███▎      | 3/9 [00:01<00:01,  4.09it/s, loss=2.26]Epoch 0:  44%|████▍     | 4/9 [00:01<00:01,  4.65it/s, loss=2.26]Epoch 0:  44%|████▍     | 4/9 [00:01<00:01,  4.65it/s, loss=2.26]Epoch 0:  44%|████▍     | 4/9 [00:01<00:01,  4.65it/s, loss=2.16]Epoch 0:  56%|█████▌    | 5/9 [00:01<00:00,  4.93it/s, loss=2.16]Epoch 0:  56%|█████▌    | 5/9 [00:01<00:00,  4.93it/s, loss=2.16]Epoch 0:  56%|█████▌    | 5/9 [00:01<00:00,  4.93it/s, loss=2.07]Epoch 0:  67%|██████▋   | 6/9 [00:01<00:00,  5.11it/s, loss=2.07]Epoch 0:  67%|██████▋   | 6/9 [00:01<00:00,  5.11it/s, loss=2.07]Epoch 0:  67%|██████▋   | 6/9 [00:01<00:00,  5.11it/s, loss=1.85]Epoch 0:  78%|███████▊  | 7/9 [00:01<00:00,  5.30it/s, loss=1.85]Epoch 0:  78%|███████▊  | 7/9 [00:01<00:00,  5.30it/s, loss=1.85]Epoch 0:  78%|███████▊  | 7/9 [00:01<00:00,  5.30it/s, loss=1.76]Epoch 0:  89%|████████▉ | 8/9 [00:01<00:00,  5.36it/s, loss=1.76]Epoch 0:  89%|████████▉ | 8/9 [00:01<00:00,  5.36it/s, loss=1.76]Epoch 0:  89%|████████▉ | 8/9 [00:01<00:00,  5.36it/s, loss=0.348]Epoch 0: 100%|██████████| 9/9 [00:01<00:00,  5.86it/s, loss=0.348]Epoch 0: 100%|██████████| 9/9 [00:01<00:00,  4.70it/s, loss=0.348]
2024-08-20 05:52:19,938 - INFO - Epoch 0: Last Batch Loss = 0.3478783071041107
2024-08-20 05:52:21,324 - INFO - Evaluation result at epoch 1:
	   post_l1_fact_eng  post_l1_fact_orig  post_l2_fact_eng  post_l2_fact_orig
	0              0.88               0.90              0.90               0.90
	1              0.92               0.92              0.94               0.94
	2              0.98               0.98              1.00               1.00
	3              1.00               1.00              1.00               1.00
	4              1.00               1.00              1.00               1.00
  0%|          | 0/9 [00:00<?, ?it/s]Epoch 1:   0%|          | 0/9 [00:00<?, ?it/s]Epoch 1:   0%|          | 0/9 [00:00<?, ?it/s, loss=1.11]Epoch 1:  11%|█         | 1/9 [00:00<00:03,  2.53it/s, loss=1.11]Epoch 1:  11%|█         | 1/9 [00:00<00:03,  2.53it/s, loss=1.11]Epoch 1:  11%|█         | 1/9 [00:00<00:03,  2.53it/s, loss=1.12]Epoch 1:  22%|██▏       | 2/9 [00:00<00:01,  3.82it/s, loss=1.12]Epoch 1:  22%|██▏       | 2/9 [00:00<00:01,  3.82it/s, loss=1.12]Epoch 1:  22%|██▏       | 2/9 [00:00<00:01,  3.82it/s, loss=1.13]Epoch 1:  33%|███▎      | 3/9 [00:00<00:01,  4.52it/s, loss=1.13]Epoch 1:  33%|███▎      | 3/9 [00:00<00:01,  4.52it/s, loss=1.13]Epoch 1:  33%|███▎      | 3/9 [00:00<00:01,  4.52it/s, loss=0.897]Epoch 1:  44%|████▍     | 4/9 [00:00<00:01,  4.73it/s, loss=0.897]Epoch 1:  44%|████▍     | 4/9 [00:01<00:01,  4.73it/s, loss=0.897]Epoch 1:  44%|████▍     | 4/9 [00:01<00:01,  4.73it/s, loss=0.963]Epoch 1:  56%|█████▌    | 5/9 [00:01<00:00,  5.07it/s, loss=0.963]Epoch 1:  56%|█████▌    | 5/9 [00:01<00:00,  5.07it/s, loss=0.963]Epoch 1:  56%|█████▌    | 5/9 [00:01<00:00,  5.07it/s, loss=0.905]Epoch 1:  67%|██████▋   | 6/9 [00:01<00:00,  5.25it/s, loss=0.905]Epoch 1:  67%|██████▋   | 6/9 [00:01<00:00,  5.25it/s, loss=0.905]Epoch 1:  67%|██████▋   | 6/9 [00:01<00:00,  5.25it/s, loss=0.963]Epoch 1:  78%|███████▊  | 7/9 [00:01<00:00,  5.39it/s, loss=0.963]Epoch 1:  78%|███████▊  | 7/9 [00:01<00:00,  5.39it/s, loss=0.963]Epoch 1:  78%|███████▊  | 7/9 [00:01<00:00,  5.39it/s, loss=0.945]Epoch 1:  89%|████████▉ | 8/9 [00:01<00:00,  5.40it/s, loss=0.945]Epoch 1:  89%|████████▉ | 8/9 [00:01<00:00,  5.40it/s, loss=0.945]Epoch 1:  89%|████████▉ | 8/9 [00:01<00:00,  5.40it/s, loss=0.0609]Epoch 1: 100%|██████████| 9/9 [00:01<00:00,  5.91it/s, loss=0.0609]Epoch 1: 100%|██████████| 9/9 [00:01<00:00,  4.92it/s, loss=0.0609]
2024-08-20 05:52:23,157 - INFO - Epoch 1: Last Batch Loss = 0.06085125356912613
2024-08-20 05:52:24,561 - INFO - Evaluation result at epoch 2:
	   post_l1_fact_eng  post_l1_fact_orig  post_l2_fact_eng  post_l2_fact_orig
	0              0.90               0.90              0.90               0.90
	1              0.94               0.94              0.94               0.94
	2              0.98               0.98              0.98               0.98
	3              1.00               1.00              1.00               1.00
	4              1.00               1.00              1.00               1.00
Running experiment with feature_type=4, agg_type=1 (LINEAR), batch_size=16, lr=1e-4
2024-08-20 05:52:29,798 - INFO - Use pytorch device_name: cuda
2024-08-20 05:52:29,799 - INFO - Load pretrained SentenceTransformer: sentence-transformers/use-cmlm-multilingual
Some weights of the model checkpoint at sentence-transformers/use-cmlm-multilingual were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2024-08-20 05:52:31,370 - INFO - Use pytorch device_name: cuda
2024-08-20 05:52:31,370 - INFO - Load pretrained SentenceTransformer: sentence-transformers/use-cmlm-multilingual
Some weights of the model checkpoint at sentence-transformers/use-cmlm-multilingual were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Seed set to 447
{'feature_type': <FeatureSet.SBERT_EXT1_EXT2: 4>, 'agg_type': <AggregationMethod.LINEAR: 1>, 'nheads': 0, 'model_name': 'sentence-transformers/use-cmlm-multilingual', 'external_dim1': 1536, 'external_dim2': 3072, 'device': 'cuda:0', 'seed': 447, 'train_fact_path': './sample_data/trial_fact_checks.csv', 'train_post_path': './sample_data/trial_posts.csv', 'train_post2fact_mapping': './sample_data/trial_data_mapping.csv', 'train_fact_orig_emb_path': './openai-op/eval_orig-fact.pkl', 'train_fact_eng_emb_path': './openai-op/eval_eng-fact.pkl', 'train_post_l1_emb_path': './openai-op/eval_l1-post.pkl', 'train_post_l2_emb_path': './openai-op/eval_l2-post.pkl', 'eval_fact_path': './sample_data/trial_fact_checks.csv', 'eval_post_path': './sample_data/trial_posts.csv', 'eval_post2fact_mapping': './sample_data/trial_data_mapping.csv', 'eval_fact_orig_emb_path': './openai-op/eval_orig-fact.pkl', 'eval_fact_eng_emb_path': './openai-op/eval_eng-fact.pkl', 'eval_post_l1_emb_path': './openai-op/eval_l1-post.pkl', 'eval_post_l2_emb_path': './openai-op/eval_l2-post.pkl', 'loss_scale': 20, 'train_batch_size': 16, 'accumulation_steps': 1, 'eval_batch_size': 16, 'max_seq_len': 312, 'n_epochs': 2, 'wt_decay': 1e-08, 'lr': 0.0001, 'K': 5, 'log_path': './logs/447/f-SBERT_EXT1_EXT2_a-LINEAR_b-16_nh-0.log', 'checkpoint_dir': './checkpoints/', 'model_checkpointing': False, 'load_checkpoint': False, 'checkpoint_path': './checkpoints/checkpoint_epoch_1.pth', 'loss_log_step': -1}
0it [00:00, ?it/s]50it [00:00, 17766.45it/s]
/home/ubuntu/miniconda3/envs/mlingual/lib/python3.11/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
2024-08-20 05:52:35,025 - INFO - Evaluation result at epoch 0:
	   post_l1_fact_eng  post_l1_fact_orig  post_l2_fact_eng  post_l2_fact_orig
	0              0.66               0.64              0.68               0.62
	1              0.74               0.74              0.82               0.76
	2              0.78               0.78              0.84               0.80
	3              0.80               0.78              0.84               0.82
	4              0.84               0.80              0.84               0.84
  0%|          | 0/9 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/9 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/9 [00:00<?, ?it/s, loss=1.41]Epoch 0:  11%|█         | 1/9 [00:00<00:03,  2.11it/s, loss=1.41]Epoch 0:  11%|█         | 1/9 [00:00<00:03,  2.11it/s, loss=1.41]Epoch 0:  11%|█         | 1/9 [00:00<00:03,  2.11it/s, loss=0.889]Epoch 0:  22%|██▏       | 2/9 [00:00<00:02,  3.32it/s, loss=0.889]Epoch 0:  22%|██▏       | 2/9 [00:00<00:02,  3.32it/s, loss=0.889]Epoch 0:  22%|██▏       | 2/9 [00:00<00:02,  3.32it/s, loss=0.991]Epoch 0:  33%|███▎      | 3/9 [00:00<00:01,  4.12it/s, loss=0.991]Epoch 0:  33%|███▎      | 3/9 [00:01<00:01,  4.12it/s, loss=0.991]Epoch 0:  33%|███▎      | 3/9 [00:01<00:01,  4.12it/s, loss=0.432]Epoch 0:  44%|████▍     | 4/9 [00:01<00:01,  4.59it/s, loss=0.432]Epoch 0:  44%|████▍     | 4/9 [00:01<00:01,  4.59it/s, loss=0.432]Epoch 0:  44%|████▍     | 4/9 [00:01<00:01,  4.59it/s, loss=0.866]Epoch 0:  56%|█████▌    | 5/9 [00:01<00:00,  4.87it/s, loss=0.866]Epoch 0:  56%|█████▌    | 5/9 [00:01<00:00,  4.87it/s, loss=0.866]Epoch 0:  56%|█████▌    | 5/9 [00:01<00:00,  4.87it/s, loss=0.435]Epoch 0:  67%|██████▋   | 6/9 [00:01<00:00,  5.23it/s, loss=0.435]Epoch 0:  67%|██████▋   | 6/9 [00:01<00:00,  5.23it/s, loss=0.435]Epoch 0:  67%|██████▋   | 6/9 [00:01<00:00,  5.23it/s, loss=0.215]Epoch 0:  78%|███████▊  | 7/9 [00:01<00:00,  5.34it/s, loss=0.215]Epoch 0:  78%|███████▊  | 7/9 [00:01<00:00,  5.34it/s, loss=0.215]Epoch 0:  78%|███████▊  | 7/9 [00:01<00:00,  5.34it/s, loss=0.169]Epoch 0:  89%|████████▉ | 8/9 [00:01<00:00,  5.40it/s, loss=0.169]Epoch 0:  89%|████████▉ | 8/9 [00:01<00:00,  5.40it/s, loss=0.169]Epoch 0:  89%|████████▉ | 8/9 [00:01<00:00,  5.40it/s, loss=6.69e-5]Epoch 0: 100%|██████████| 9/9 [00:01<00:00,  5.90it/s, loss=6.69e-5]Epoch 0: 100%|██████████| 9/9 [00:01<00:00,  4.74it/s, loss=6.69e-5]
2024-08-20 05:52:36,926 - INFO - Epoch 0: Last Batch Loss = 6.687335553579032e-05
2024-08-20 05:52:38,303 - INFO - Evaluation result at epoch 1:
	   post_l1_fact_eng  post_l1_fact_orig  post_l2_fact_eng  post_l2_fact_orig
	0              0.94               0.94              0.94               0.94
	1              0.96               0.96              0.96               0.98
	2              1.00               1.00              1.00               1.00
	3              1.00               1.00              1.00               1.00
	4              1.00               1.00              1.00               1.00
  0%|          | 0/9 [00:00<?, ?it/s]Epoch 1:   0%|          | 0/9 [00:00<?, ?it/s]Epoch 1:   0%|          | 0/9 [00:00<?, ?it/s, loss=0.319]Epoch 1:  11%|█         | 1/9 [00:00<00:03,  2.58it/s, loss=0.319]Epoch 1:  11%|█         | 1/9 [00:00<00:03,  2.58it/s, loss=0.319]Epoch 1:  11%|█         | 1/9 [00:00<00:03,  2.58it/s, loss=0.184]Epoch 1:  22%|██▏       | 2/9 [00:00<00:01,  3.73it/s, loss=0.184]Epoch 1:  22%|██▏       | 2/9 [00:00<00:01,  3.73it/s, loss=0.184]Epoch 1:  22%|██▏       | 2/9 [00:00<00:01,  3.73it/s, loss=0.293]Epoch 1:  33%|███▎      | 3/9 [00:00<00:01,  4.45it/s, loss=0.293]Epoch 1:  33%|███▎      | 3/9 [00:00<00:01,  4.45it/s, loss=0.293]Epoch 1:  33%|███▎      | 3/9 [00:00<00:01,  4.45it/s, loss=0.331]Epoch 1:  44%|████▍     | 4/9 [00:00<00:01,  4.72it/s, loss=0.331]Epoch 1:  44%|████▍     | 4/9 [00:01<00:01,  4.72it/s, loss=0.331]Epoch 1:  44%|████▍     | 4/9 [00:01<00:01,  4.72it/s, loss=0.427]Epoch 1:  56%|█████▌    | 5/9 [00:01<00:00,  5.13it/s, loss=0.427]Epoch 1:  56%|█████▌    | 5/9 [00:01<00:00,  5.13it/s, loss=0.427]Epoch 1:  56%|█████▌    | 5/9 [00:01<00:00,  5.13it/s, loss=0.252]Epoch 1:  67%|██████▋   | 6/9 [00:01<00:00,  5.39it/s, loss=0.252]Epoch 1:  67%|██████▋   | 6/9 [00:01<00:00,  5.39it/s, loss=0.252]Epoch 1:  67%|██████▋   | 6/9 [00:01<00:00,  5.39it/s, loss=0.0424]Epoch 1:  78%|███████▊  | 7/9 [00:01<00:00,  5.46it/s, loss=0.0424]Epoch 1:  78%|███████▊  | 7/9 [00:01<00:00,  5.46it/s, loss=0.0424]Epoch 1:  78%|███████▊  | 7/9 [00:01<00:00,  5.46it/s, loss=0.227] Epoch 1:  89%|████████▉ | 8/9 [00:01<00:00,  5.44it/s, loss=0.227]Epoch 1:  89%|████████▉ | 8/9 [00:01<00:00,  5.44it/s, loss=0.227]Epoch 1:  89%|████████▉ | 8/9 [00:01<00:00,  5.44it/s, loss=0.000295]Epoch 1: 100%|██████████| 9/9 [00:01<00:00,  5.97it/s, loss=0.000295]Epoch 1: 100%|██████████| 9/9 [00:01<00:00,  4.95it/s, loss=0.000295]
2024-08-20 05:52:40,125 - INFO - Epoch 1: Last Batch Loss = 0.0002945506712421775
2024-08-20 05:52:41,499 - INFO - Evaluation result at epoch 2:
	   post_l1_fact_eng  post_l1_fact_orig  post_l2_fact_eng  post_l2_fact_orig
	0              0.94               0.94              0.94               0.94
	1              0.96               0.98              0.98               0.98
	2              1.00               1.00              1.00               1.00
	3              1.00               1.00              1.00               1.00
	4              1.00               1.00              1.00               1.00
Running experiment with feature_type=4, agg_type=2 (ATTENTION), nheads=4, batch_size=16, lr=1e-4
2024-08-20 05:52:46,752 - INFO - Use pytorch device_name: cuda
2024-08-20 05:52:46,752 - INFO - Load pretrained SentenceTransformer: sentence-transformers/use-cmlm-multilingual
Some weights of the model checkpoint at sentence-transformers/use-cmlm-multilingual were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2024-08-20 05:52:48,326 - INFO - Use pytorch device_name: cuda
2024-08-20 05:52:48,327 - INFO - Load pretrained SentenceTransformer: sentence-transformers/use-cmlm-multilingual
Some weights of the model checkpoint at sentence-transformers/use-cmlm-multilingual were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Seed set to 447
{'feature_type': <FeatureSet.SBERT_EXT1_EXT2: 4>, 'agg_type': <AggregationMethod.ATTENTION: 2>, 'nheads': 4, 'model_name': 'sentence-transformers/use-cmlm-multilingual', 'external_dim1': 1536, 'external_dim2': 3072, 'device': 'cuda:0', 'seed': 447, 'train_fact_path': './sample_data/trial_fact_checks.csv', 'train_post_path': './sample_data/trial_posts.csv', 'train_post2fact_mapping': './sample_data/trial_data_mapping.csv', 'train_fact_orig_emb_path': './openai-op/eval_orig-fact.pkl', 'train_fact_eng_emb_path': './openai-op/eval_eng-fact.pkl', 'train_post_l1_emb_path': './openai-op/eval_l1-post.pkl', 'train_post_l2_emb_path': './openai-op/eval_l2-post.pkl', 'eval_fact_path': './sample_data/trial_fact_checks.csv', 'eval_post_path': './sample_data/trial_posts.csv', 'eval_post2fact_mapping': './sample_data/trial_data_mapping.csv', 'eval_fact_orig_emb_path': './openai-op/eval_orig-fact.pkl', 'eval_fact_eng_emb_path': './openai-op/eval_eng-fact.pkl', 'eval_post_l1_emb_path': './openai-op/eval_l1-post.pkl', 'eval_post_l2_emb_path': './openai-op/eval_l2-post.pkl', 'loss_scale': 20, 'train_batch_size': 16, 'accumulation_steps': 1, 'eval_batch_size': 16, 'max_seq_len': 312, 'n_epochs': 2, 'wt_decay': 1e-08, 'lr': 0.0001, 'K': 5, 'log_path': './logs/447/f-SBERT_EXT1_EXT2_a-ATTENTION_b-16_nh-4.log', 'checkpoint_dir': './checkpoints/', 'model_checkpointing': False, 'load_checkpoint': False, 'checkpoint_path': './checkpoints/checkpoint_epoch_1.pth', 'loss_log_step': -1}
0it [00:00, ?it/s]50it [00:00, 17811.72it/s]
/home/ubuntu/miniconda3/envs/mlingual/lib/python3.11/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
2024-08-20 05:52:51,964 - INFO - Evaluation result at epoch 0:
	   post_l1_fact_eng  post_l1_fact_orig  post_l2_fact_eng  post_l2_fact_orig
	0              0.58               0.56              0.64               0.60
	1              0.68               0.60              0.68               0.66
	2              0.74               0.72              0.74               0.72
	3              0.78               0.78              0.84               0.80
	4              0.84               0.82              0.84               0.84
  0%|          | 0/9 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/9 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/9 [00:00<?, ?it/s, loss=2.64]Epoch 0:  11%|█         | 1/9 [00:00<00:03,  2.18it/s, loss=2.64]Epoch 0:  11%|█         | 1/9 [00:00<00:03,  2.18it/s, loss=2.64]Epoch 0:  11%|█         | 1/9 [00:00<00:03,  2.18it/s, loss=2.66]Epoch 0:  22%|██▏       | 2/9 [00:00<00:02,  3.31it/s, loss=2.66]Epoch 0:  22%|██▏       | 2/9 [00:00<00:02,  3.31it/s, loss=2.66]Epoch 0:  22%|██▏       | 2/9 [00:00<00:02,  3.31it/s, loss=2.61]Epoch 0:  33%|███▎      | 3/9 [00:00<00:01,  4.10it/s, loss=2.61]Epoch 0:  33%|███▎      | 3/9 [00:00<00:01,  4.10it/s, loss=2.61]Epoch 0:  33%|███▎      | 3/9 [00:00<00:01,  4.10it/s, loss=2.48]Epoch 0:  44%|████▍     | 4/9 [00:00<00:01,  4.68it/s, loss=2.48]Epoch 0:  44%|████▍     | 4/9 [00:01<00:01,  4.68it/s, loss=2.48]Epoch 0:  44%|████▍     | 4/9 [00:01<00:01,  4.68it/s, loss=2.44]Epoch 0:  56%|█████▌    | 5/9 [00:01<00:00,  4.95it/s, loss=2.44]Epoch 0:  56%|█████▌    | 5/9 [00:01<00:00,  4.95it/s, loss=2.44]Epoch 0:  56%|█████▌    | 5/9 [00:01<00:00,  4.95it/s, loss=2.38]Epoch 0:  67%|██████▋   | 6/9 [00:01<00:00,  5.13it/s, loss=2.38]Epoch 0:  67%|██████▋   | 6/9 [00:01<00:00,  5.13it/s, loss=2.38]Epoch 0:  67%|██████▋   | 6/9 [00:01<00:00,  5.13it/s, loss=2.25]Epoch 0:  78%|███████▊  | 7/9 [00:01<00:00,  5.31it/s, loss=2.25]Epoch 0:  78%|███████▊  | 7/9 [00:01<00:00,  5.31it/s, loss=2.25]Epoch 0:  78%|███████▊  | 7/9 [00:01<00:00,  5.31it/s, loss=2.16]Epoch 0:  89%|████████▉ | 8/9 [00:01<00:00,  5.37it/s, loss=2.16]Epoch 0:  89%|████████▉ | 8/9 [00:01<00:00,  5.37it/s, loss=2.16]Epoch 0:  89%|████████▉ | 8/9 [00:01<00:00,  5.37it/s, loss=0.462]Epoch 0: 100%|██████████| 9/9 [00:01<00:00,  5.87it/s, loss=0.462]Epoch 0: 100%|██████████| 9/9 [00:01<00:00,  4.74it/s, loss=0.462]
2024-08-20 05:52:53,865 - INFO - Epoch 0: Last Batch Loss = 0.46179884672164917
2024-08-20 05:52:55,252 - INFO - Evaluation result at epoch 1:
	   post_l1_fact_eng  post_l1_fact_orig  post_l2_fact_eng  post_l2_fact_orig
	0              0.86               0.88              0.90               0.90
	1              0.94               0.92              0.96               0.96
	2              0.96               0.94              0.98               0.98
	3              1.00               1.00              1.00               1.00
	4              1.00               1.00              1.00               1.00
  0%|          | 0/9 [00:00<?, ?it/s]Epoch 1:   0%|          | 0/9 [00:00<?, ?it/s]Epoch 1:   0%|          | 0/9 [00:00<?, ?it/s, loss=1.61]Epoch 1:  11%|█         | 1/9 [00:00<00:03,  2.53it/s, loss=1.61]Epoch 1:  11%|█         | 1/9 [00:00<00:03,  2.53it/s, loss=1.61]Epoch 1:  11%|█         | 1/9 [00:00<00:03,  2.53it/s, loss=1.6] Epoch 1:  22%|██▏       | 2/9 [00:00<00:01,  3.81it/s, loss=1.6]Epoch 1:  22%|██▏       | 2/9 [00:00<00:01,  3.81it/s, loss=1.6]Epoch 1:  22%|██▏       | 2/9 [00:00<00:01,  3.81it/s, loss=1.58]Epoch 1:  33%|███▎      | 3/9 [00:00<00:01,  4.51it/s, loss=1.58]Epoch 1:  33%|███▎      | 3/9 [00:00<00:01,  4.51it/s, loss=1.58]Epoch 1:  33%|███▎      | 3/9 [00:00<00:01,  4.51it/s, loss=1.36]Epoch 1:  44%|████▍     | 4/9 [00:00<00:01,  4.72it/s, loss=1.36]Epoch 1:  44%|████▍     | 4/9 [00:01<00:01,  4.72it/s, loss=1.36]Epoch 1:  44%|████▍     | 4/9 [00:01<00:01,  4.72it/s, loss=1.38]Epoch 1:  56%|█████▌    | 5/9 [00:01<00:00,  5.06it/s, loss=1.38]Epoch 1:  56%|█████▌    | 5/9 [00:01<00:00,  5.06it/s, loss=1.38]Epoch 1:  56%|█████▌    | 5/9 [00:01<00:00,  5.06it/s, loss=1.34]Epoch 1:  67%|██████▋   | 6/9 [00:01<00:00,  5.24it/s, loss=1.34]Epoch 1:  67%|██████▋   | 6/9 [00:01<00:00,  5.24it/s, loss=1.34]Epoch 1:  67%|██████▋   | 6/9 [00:01<00:00,  5.24it/s, loss=1.46]Epoch 1:  78%|███████▊  | 7/9 [00:01<00:00,  5.39it/s, loss=1.46]Epoch 1:  78%|███████▊  | 7/9 [00:01<00:00,  5.39it/s, loss=1.46]Epoch 1:  78%|███████▊  | 7/9 [00:01<00:00,  5.39it/s, loss=1.35]Epoch 1:  89%|████████▉ | 8/9 [00:01<00:00,  5.40it/s, loss=1.35]Epoch 1:  89%|████████▉ | 8/9 [00:01<00:00,  5.40it/s, loss=1.35]Epoch 1:  89%|████████▉ | 8/9 [00:01<00:00,  5.40it/s, loss=0.132]Epoch 1: 100%|██████████| 9/9 [00:01<00:00,  5.90it/s, loss=0.132]Epoch 1: 100%|██████████| 9/9 [00:01<00:00,  4.91it/s, loss=0.132]
2024-08-20 05:52:57,090 - INFO - Epoch 1: Last Batch Loss = 0.1322420835494995
2024-08-20 05:52:58,473 - INFO - Evaluation result at epoch 2:
	   post_l1_fact_eng  post_l1_fact_orig  post_l2_fact_eng  post_l2_fact_orig
	0              0.86               0.86              0.90               0.90
	1              0.90               0.88              0.94               0.94
	2              0.94               0.96              0.96               0.96
	3              0.96               0.96              0.96               0.96
	4              1.00               1.00              1.00               1.00
Running experiment with feature_type=4, agg_type=2 (ATTENTION), nheads=8, batch_size=16, lr=1e-4
2024-08-20 05:53:03,686 - INFO - Use pytorch device_name: cuda
2024-08-20 05:53:03,686 - INFO - Load pretrained SentenceTransformer: sentence-transformers/use-cmlm-multilingual
Some weights of the model checkpoint at sentence-transformers/use-cmlm-multilingual were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2024-08-20 05:53:05,336 - INFO - Use pytorch device_name: cuda
2024-08-20 05:53:05,336 - INFO - Load pretrained SentenceTransformer: sentence-transformers/use-cmlm-multilingual
Some weights of the model checkpoint at sentence-transformers/use-cmlm-multilingual were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Seed set to 447
{'feature_type': <FeatureSet.SBERT_EXT1_EXT2: 4>, 'agg_type': <AggregationMethod.ATTENTION: 2>, 'nheads': 8, 'model_name': 'sentence-transformers/use-cmlm-multilingual', 'external_dim1': 1536, 'external_dim2': 3072, 'device': 'cuda:0', 'seed': 447, 'train_fact_path': './sample_data/trial_fact_checks.csv', 'train_post_path': './sample_data/trial_posts.csv', 'train_post2fact_mapping': './sample_data/trial_data_mapping.csv', 'train_fact_orig_emb_path': './openai-op/eval_orig-fact.pkl', 'train_fact_eng_emb_path': './openai-op/eval_eng-fact.pkl', 'train_post_l1_emb_path': './openai-op/eval_l1-post.pkl', 'train_post_l2_emb_path': './openai-op/eval_l2-post.pkl', 'eval_fact_path': './sample_data/trial_fact_checks.csv', 'eval_post_path': './sample_data/trial_posts.csv', 'eval_post2fact_mapping': './sample_data/trial_data_mapping.csv', 'eval_fact_orig_emb_path': './openai-op/eval_orig-fact.pkl', 'eval_fact_eng_emb_path': './openai-op/eval_eng-fact.pkl', 'eval_post_l1_emb_path': './openai-op/eval_l1-post.pkl', 'eval_post_l2_emb_path': './openai-op/eval_l2-post.pkl', 'loss_scale': 20, 'train_batch_size': 16, 'accumulation_steps': 1, 'eval_batch_size': 16, 'max_seq_len': 312, 'n_epochs': 2, 'wt_decay': 1e-08, 'lr': 0.0001, 'K': 5, 'log_path': './logs/447/f-SBERT_EXT1_EXT2_a-ATTENTION_b-16_nh-8.log', 'checkpoint_dir': './checkpoints/', 'model_checkpointing': False, 'load_checkpoint': False, 'checkpoint_path': './checkpoints/checkpoint_epoch_1.pth', 'loss_log_step': -1}
0it [00:00, ?it/s]50it [00:00, 17728.90it/s]
/home/ubuntu/miniconda3/envs/mlingual/lib/python3.11/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
2024-08-20 05:53:09,007 - INFO - Evaluation result at epoch 0:
	   post_l1_fact_eng  post_l1_fact_orig  post_l2_fact_eng  post_l2_fact_orig
	0              0.58               0.56              0.64               0.60
	1              0.68               0.60              0.68               0.66
	2              0.74               0.72              0.74               0.72
	3              0.78               0.78              0.84               0.80
	4              0.84               0.82              0.84               0.84
  0%|          | 0/9 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/9 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/9 [00:00<?, ?it/s, loss=2.64]Epoch 0:  11%|█         | 1/9 [00:00<00:03,  2.16it/s, loss=2.64]Epoch 0:  11%|█         | 1/9 [00:00<00:03,  2.16it/s, loss=2.64]Epoch 0:  11%|█         | 1/9 [00:00<00:03,  2.16it/s, loss=2.66]Epoch 0:  22%|██▏       | 2/9 [00:00<00:02,  3.36it/s, loss=2.66]Epoch 0:  22%|██▏       | 2/9 [00:00<00:02,  3.36it/s, loss=2.66]Epoch 0:  22%|██▏       | 2/9 [00:00<00:02,  3.36it/s, loss=2.61]Epoch 0:  33%|███▎      | 3/9 [00:00<00:01,  4.11it/s, loss=2.61]Epoch 0:  33%|███▎      | 3/9 [00:00<00:01,  4.11it/s, loss=2.61]Epoch 0:  33%|███▎      | 3/9 [00:00<00:01,  4.11it/s, loss=2.48]Epoch 0:  44%|████▍     | 4/9 [00:00<00:01,  4.62it/s, loss=2.48]Epoch 0:  44%|████▍     | 4/9 [00:01<00:01,  4.62it/s, loss=2.48]Epoch 0:  44%|████▍     | 4/9 [00:01<00:01,  4.62it/s, loss=2.44]Epoch 0:  56%|█████▌    | 5/9 [00:01<00:00,  4.90it/s, loss=2.44]Epoch 0:  56%|█████▌    | 5/9 [00:01<00:00,  4.90it/s, loss=2.44]Epoch 0:  56%|█████▌    | 5/9 [00:01<00:00,  4.90it/s, loss=2.38]Epoch 0:  67%|██████▋   | 6/9 [00:01<00:00,  5.09it/s, loss=2.38]Epoch 0:  67%|██████▋   | 6/9 [00:01<00:00,  5.09it/s, loss=2.38]Epoch 0:  67%|██████▋   | 6/9 [00:01<00:00,  5.09it/s, loss=2.25]Epoch 0:  78%|███████▊  | 7/9 [00:01<00:00,  5.28it/s, loss=2.25]Epoch 0:  78%|███████▊  | 7/9 [00:01<00:00,  5.28it/s, loss=2.25]Epoch 0:  78%|███████▊  | 7/9 [00:01<00:00,  5.28it/s, loss=2.16]Epoch 0:  89%|████████▉ | 8/9 [00:01<00:00,  5.34it/s, loss=2.16]Epoch 0:  89%|████████▉ | 8/9 [00:01<00:00,  5.34it/s, loss=2.16]Epoch 0:  89%|████████▉ | 8/9 [00:01<00:00,  5.34it/s, loss=0.462]Epoch 0: 100%|██████████| 9/9 [00:01<00:00,  5.85it/s, loss=0.462]Epoch 0: 100%|██████████| 9/9 [00:01<00:00,  4.71it/s, loss=0.462]
2024-08-20 05:53:10,919 - INFO - Epoch 0: Last Batch Loss = 0.46182164549827576
2024-08-20 05:53:12,317 - INFO - Evaluation result at epoch 1:
	   post_l1_fact_eng  post_l1_fact_orig  post_l2_fact_eng  post_l2_fact_orig
	0              0.86               0.88              0.90               0.90
	1              0.94               0.92              0.96               0.96
	2              0.96               0.94              0.98               0.98
	3              1.00               1.00              1.00               1.00
	4              1.00               1.00              1.00               1.00
  0%|          | 0/9 [00:00<?, ?it/s]Epoch 1:   0%|          | 0/9 [00:00<?, ?it/s]Epoch 1:   0%|          | 0/9 [00:00<?, ?it/s, loss=1.61]Epoch 1:  11%|█         | 1/9 [00:00<00:03,  2.52it/s, loss=1.61]Epoch 1:  11%|█         | 1/9 [00:00<00:03,  2.52it/s, loss=1.61]Epoch 1:  11%|█         | 1/9 [00:00<00:03,  2.52it/s, loss=1.6] Epoch 1:  22%|██▏       | 2/9 [00:00<00:01,  3.84it/s, loss=1.6]Epoch 1:  22%|██▏       | 2/9 [00:00<00:01,  3.84it/s, loss=1.6]Epoch 1:  22%|██▏       | 2/9 [00:00<00:01,  3.84it/s, loss=1.58]Epoch 1:  33%|███▎      | 3/9 [00:00<00:01,  4.53it/s, loss=1.58]Epoch 1:  33%|███▎      | 3/9 [00:00<00:01,  4.53it/s, loss=1.58]Epoch 1:  33%|███▎      | 3/9 [00:00<00:01,  4.53it/s, loss=1.36]Epoch 1:  44%|████▍     | 4/9 [00:00<00:01,  4.75it/s, loss=1.36]Epoch 1:  44%|████▍     | 4/9 [00:01<00:01,  4.75it/s, loss=1.36]Epoch 1:  44%|████▍     | 4/9 [00:01<00:01,  4.75it/s, loss=1.38]Epoch 1:  56%|█████▌    | 5/9 [00:01<00:00,  5.09it/s, loss=1.38]Epoch 1:  56%|█████▌    | 5/9 [00:01<00:00,  5.09it/s, loss=1.38]Epoch 1:  56%|█████▌    | 5/9 [00:01<00:00,  5.09it/s, loss=1.34]Epoch 1:  67%|██████▋   | 6/9 [00:01<00:00,  5.26it/s, loss=1.34]Epoch 1:  67%|██████▋   | 6/9 [00:01<00:00,  5.26it/s, loss=1.34]Epoch 1:  67%|██████▋   | 6/9 [00:01<00:00,  5.26it/s, loss=1.46]Epoch 1:  78%|███████▊  | 7/9 [00:01<00:00,  5.40it/s, loss=1.46]Epoch 1:  78%|███████▊  | 7/9 [00:01<00:00,  5.40it/s, loss=1.46]Epoch 1:  78%|███████▊  | 7/9 [00:01<00:00,  5.40it/s, loss=1.35]Epoch 1:  89%|████████▉ | 8/9 [00:01<00:00,  5.41it/s, loss=1.35]Epoch 1:  89%|████████▉ | 8/9 [00:01<00:00,  5.41it/s, loss=1.35]Epoch 1:  89%|████████▉ | 8/9 [00:01<00:00,  5.41it/s, loss=0.132]Epoch 1: 100%|██████████| 9/9 [00:01<00:00,  5.92it/s, loss=0.132]Epoch 1: 100%|██████████| 9/9 [00:01<00:00,  4.93it/s, loss=0.132]
2024-08-20 05:53:14,144 - INFO - Epoch 1: Last Batch Loss = 0.13223840296268463
2024-08-20 05:53:15,526 - INFO - Evaluation result at epoch 2:
	   post_l1_fact_eng  post_l1_fact_orig  post_l2_fact_eng  post_l2_fact_orig
	0              0.86               0.86              0.90               0.90
	1              0.90               0.88              0.94               0.94
	2              0.94               0.96              0.96               0.96
	3              0.96               0.96              0.96               0.96
	4              1.00               1.00              1.00               1.00
